{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImfGgUkO5nyr",
        "outputId": "dae3037a-19e9-4c20-d882-cffc564dc93e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kornia in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: kornia_rs>=0.1.9 in /usr/local/lib/python3.11/dist-packages (from kornia) (0.1.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kornia) (24.2)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from kornia) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.1->kornia) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.1->kornia) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install kornia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8H_D-XgqT5w"
      },
      "outputs": [],
      "source": [
        "import os, glob, zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import torch\n",
        "from torch import nn\n",
        "import timm\n",
        "import kagglehub\n",
        "from collections import Counter\n",
        "import kornia.augmentation as K\n",
        "import time\n",
        "from tqdm import trange, tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K64iu6UtpMXw",
        "outputId": "2205c52c-1cb6-401e-f924-e2d75e1dc6a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset path: /kaggle/input/wm811k-wafer-map\n"
          ]
        }
      ],
      "source": [
        "path = kagglehub.dataset_download(\"qingyi/wm811k-wafer-map\")\n",
        "print(\"Dataset path:\", path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoPzCyulzgf8"
      },
      "outputs": [],
      "source": [
        "# 1) Point to the folder where the .pkl is extracted\n",
        "DATA_DIR = \"/kaggle/input/wm811k-wafer-map\"\n",
        "pkl_file = glob.glob(os.path.join(DATA_DIR, \"*.pkl\"))[0]\n",
        "\n",
        "# 2) Load DataFrame\n",
        "df = pd.read_pickle(pkl_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImaU6__mqR7g",
        "outputId": "5fafb2c9-61ea-4610-b3bf-d50b9e56968c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall label counts: Counter({np.int64(0): 785938, np.int64(1): 25519})\n",
            "Train dist: Counter({np.int64(0): 628750, np.int64(1): 20415}) Val dist: Counter({np.int64(0): 157188, np.int64(1): 5104})\n"
          ]
        }
      ],
      "source": [
        "# ── 2) Robustly pre‐stack wafer maps into a single (N, 52, 52) array ──\n",
        "maps_raw = df[\"waferMap\"].values\n",
        "all_maps = []\n",
        "for m in maps_raw:\n",
        "    arr = np.array(m)\n",
        "\n",
        "    # If there's an extra singleton channel dimension, drop it\n",
        "    if arr.ndim == 3 and arr.shape[2] == 1:\n",
        "        arr = arr[:, :, 0]\n",
        "\n",
        "    # If flat 1D list, infer side and reshape\n",
        "    if arr.ndim == 1:\n",
        "        side = int(np.sqrt(arr.size))\n",
        "        arr = arr.reshape(side, side)\n",
        "\n",
        "    # If 2D but non‐square, center‐crop to square\n",
        "    elif arr.ndim == 2:\n",
        "        H, W = arr.shape\n",
        "        if H != W:\n",
        "            side = min(H, W)\n",
        "            top  = (H - side) // 2\n",
        "            left = (W - side) // 2\n",
        "            arr  = arr[top:top+side, left:left+side]\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported ndim={arr.ndim}\")\n",
        "\n",
        "    # If not exactly 52×52, resize with nearest‐neighbor\n",
        "    if arr.shape != (52, 52):\n",
        "        from PIL import Image as PILImage\n",
        "        pil = PILImage.fromarray(arr.astype(np.uint8))\n",
        "        pil = pil.resize((52, 52), resample=PILImage.NEAREST)\n",
        "        arr = np.array(pil)\n",
        "\n",
        "    all_maps.append(arr.astype(np.uint8))\n",
        "\n",
        "# Stack into a contiguous array\n",
        "all_maps = np.stack(all_maps, axis=0)  # shape (N, 52, 52)\n",
        "\n",
        "# ── 3) Binarize failureType labels ───────────────────────────────────\n",
        "ftype_raw = df[\"failureType\"].astype(str).str.strip()\n",
        "labels = np.array([0 if s in (\"[]\", \"[['none']]\") else 1 for s in ftype_raw], dtype=np.int64)\n",
        "print(\"Overall label counts:\", Counter(labels))\n",
        "\n",
        "# ── 4) Train/Val split ───────────────────────────────────────────────\n",
        "maps_train, maps_val, lbls_train, lbls_val = train_test_split(\n",
        "    all_maps, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "print(\"Train dist:\", Counter(lbls_train), \"Val dist:\", Counter(lbls_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKUoOERMyft8",
        "outputId": "baec29a0-0953-499e-f519-aaec6bd333c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▶ Overall labels: (array([0, 1]), array([785938,  25519]))\n",
            "▶ Train labels:   (array([0, 1]), array([628750,  20415]))\n",
            "▶ Val   labels:   (array([0, 1]), array([157188,   5104]))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# after train_test_split(...)\n",
        "print(\"▶ Overall labels:\", np.unique(labels,    return_counts=True))\n",
        "print(\"▶ Train labels:  \", np.unique(lbls_train, return_counts=True))\n",
        "print(\"▶ Val   labels:  \", np.unique(lbls_val,   return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIR_evNpqXWu"
      },
      "outputs": [],
      "source": [
        "# ── 4) Dataset ──────────────────────────────────────────────────────\n",
        "class WaferMapDataset(Dataset):\n",
        "    def __init__(self, maps_array, labels, transform=None):\n",
        "        self.maps = maps_array\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.maps)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        arr = self.maps[idx]\n",
        "        img = Image.fromarray(arr * 127).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hxr4lhMm7qg",
        "outputId": "4bb61fb9-7d0b-4053-dfec-de0419562a53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 649165, batches: 5072\n",
            "Val   samples: 162292, batches: 1268\n",
            "Device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-8-3790742981.py:44: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipython-input-8-3790742981.py:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/tmp/ipython-input-8-3790742981.py:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Batch 50/5072 Loss: 0.1458 Acc: 0.950 Elapsed: 68.5s ETA: 114.7m\n",
            "[Epoch 1] Batch 100/5072 Loss: 0.1148 Acc: 0.961 Elapsed: 82.7s ETA: 68.5m\n",
            "[Epoch 1] Batch 150/5072 Loss: 0.1024 Acc: 0.965 Elapsed: 96.8s ETA: 53.0m\n",
            "[Epoch 1] Batch 200/5072 Loss: 0.0946 Acc: 0.967 Elapsed: 111.0s ETA: 45.1m\n",
            "[Epoch 1] Batch 250/5072 Loss: 0.0902 Acc: 0.969 Elapsed: 125.2s ETA: 40.3m\n",
            "[Epoch 1] Batch 300/5072 Loss: 0.0872 Acc: 0.970 Elapsed: 139.4s ETA: 37.0m\n",
            "[Epoch 1] Batch 350/5072 Loss: 0.0846 Acc: 0.971 Elapsed: 153.6s ETA: 34.5m\n",
            "[Epoch 1] Batch 400/5072 Loss: 0.0826 Acc: 0.971 Elapsed: 168.4s ETA: 32.8m\n",
            "[Epoch 1] Batch 450/5072 Loss: 0.0814 Acc: 0.972 Elapsed: 182.6s ETA: 31.3m\n",
            "[Epoch 1] Batch 500/5072 Loss: 0.0794 Acc: 0.972 Elapsed: 196.8s ETA: 30.0m\n",
            "[Epoch 1] Batch 550/5072 Loss: 0.0787 Acc: 0.972 Elapsed: 211.1s ETA: 28.9m\n",
            "[Epoch 1] Batch 600/5072 Loss: 0.0774 Acc: 0.973 Elapsed: 225.3s ETA: 28.0m\n",
            "[Epoch 1] Batch 650/5072 Loss: 0.0770 Acc: 0.973 Elapsed: 239.5s ETA: 27.2m\n",
            "[Epoch 1] Batch 700/5072 Loss: 0.0765 Acc: 0.973 Elapsed: 253.8s ETA: 26.4m\n",
            "[Epoch 1] Batch 750/5072 Loss: 0.0754 Acc: 0.973 Elapsed: 268.0s ETA: 25.7m\n",
            "[Epoch 1] Batch 800/5072 Loss: 0.0747 Acc: 0.973 Elapsed: 282.2s ETA: 25.1m\n",
            "[Epoch 1] Batch 850/5072 Loss: 0.0743 Acc: 0.973 Elapsed: 296.5s ETA: 24.5m\n",
            "[Epoch 1] Batch 900/5072 Loss: 0.0740 Acc: 0.973 Elapsed: 310.7s ETA: 24.0m\n",
            "[Epoch 1] Batch 950/5072 Loss: 0.0735 Acc: 0.974 Elapsed: 324.9s ETA: 23.5m\n",
            "[Epoch 1] Batch 1000/5072 Loss: 0.0731 Acc: 0.974 Elapsed: 339.1s ETA: 23.0m\n",
            "[Epoch 1] Batch 1050/5072 Loss: 0.0727 Acc: 0.974 Elapsed: 353.4s ETA: 22.6m\n",
            "[Epoch 1] Batch 1100/5072 Loss: 0.0724 Acc: 0.974 Elapsed: 367.6s ETA: 22.1m\n",
            "[Epoch 1] Batch 1150/5072 Loss: 0.0719 Acc: 0.974 Elapsed: 381.8s ETA: 21.7m\n",
            "[Epoch 1] Batch 1200/5072 Loss: 0.0716 Acc: 0.974 Elapsed: 396.1s ETA: 21.3m\n",
            "[Epoch 1] Batch 1250/5072 Loss: 0.0712 Acc: 0.974 Elapsed: 410.3s ETA: 20.9m\n",
            "[Epoch 1] Batch 1300/5072 Loss: 0.0711 Acc: 0.974 Elapsed: 424.5s ETA: 20.5m\n",
            "[Epoch 1] Batch 1350/5072 Loss: 0.0708 Acc: 0.974 Elapsed: 438.7s ETA: 20.2m\n",
            "[Epoch 1] Batch 1400/5072 Loss: 0.0708 Acc: 0.974 Elapsed: 453.0s ETA: 19.8m\n",
            "[Epoch 1] Batch 1450/5072 Loss: 0.0704 Acc: 0.974 Elapsed: 467.2s ETA: 19.5m\n",
            "[Epoch 1] Batch 1500/5072 Loss: 0.0699 Acc: 0.974 Elapsed: 481.4s ETA: 19.1m\n",
            "[Epoch 1] Batch 1550/5072 Loss: 0.0696 Acc: 0.975 Elapsed: 495.6s ETA: 18.8m\n",
            "[Epoch 1] Batch 1600/5072 Loss: 0.0695 Acc: 0.975 Elapsed: 509.9s ETA: 18.4m\n",
            "[Epoch 1] Batch 1650/5072 Loss: 0.0691 Acc: 0.975 Elapsed: 524.1s ETA: 18.1m\n",
            "[Epoch 1] Batch 1700/5072 Loss: 0.0688 Acc: 0.975 Elapsed: 538.3s ETA: 17.8m\n",
            "[Epoch 1] Batch 1750/5072 Loss: 0.0685 Acc: 0.975 Elapsed: 552.6s ETA: 17.5m\n",
            "[Epoch 1] Batch 1800/5072 Loss: 0.0684 Acc: 0.975 Elapsed: 566.8s ETA: 17.2m\n",
            "[Epoch 1] Batch 1850/5072 Loss: 0.0683 Acc: 0.975 Elapsed: 581.0s ETA: 16.9m\n",
            "[Epoch 1] Batch 1900/5072 Loss: 0.0685 Acc: 0.975 Elapsed: 595.2s ETA: 16.6m\n",
            "[Epoch 1] Batch 1950/5072 Loss: 0.0683 Acc: 0.975 Elapsed: 609.5s ETA: 16.3m\n",
            "[Epoch 1] Batch 2000/5072 Loss: 0.0683 Acc: 0.975 Elapsed: 623.7s ETA: 16.0m\n",
            "[Epoch 1] Batch 2050/5072 Loss: 0.0682 Acc: 0.975 Elapsed: 637.9s ETA: 15.7m\n",
            "[Epoch 1] Batch 2100/5072 Loss: 0.0680 Acc: 0.975 Elapsed: 652.2s ETA: 15.4m\n",
            "[Epoch 1] Batch 2150/5072 Loss: 0.0679 Acc: 0.975 Elapsed: 666.4s ETA: 15.1m\n",
            "[Epoch 1] Batch 2200/5072 Loss: 0.0677 Acc: 0.975 Elapsed: 680.7s ETA: 14.8m\n",
            "[Epoch 1] Batch 2250/5072 Loss: 0.0674 Acc: 0.975 Elapsed: 694.9s ETA: 14.5m\n",
            "[Epoch 1] Batch 2300/5072 Loss: 0.0672 Acc: 0.975 Elapsed: 709.1s ETA: 14.2m\n",
            "[Epoch 1] Batch 2350/5072 Loss: 0.0671 Acc: 0.975 Elapsed: 723.4s ETA: 14.0m\n",
            "[Epoch 1] Batch 2400/5072 Loss: 0.0669 Acc: 0.975 Elapsed: 737.6s ETA: 13.7m\n",
            "[Epoch 1] Batch 2450/5072 Loss: 0.0667 Acc: 0.975 Elapsed: 751.8s ETA: 13.4m\n",
            "[Epoch 1] Batch 2500/5072 Loss: 0.0665 Acc: 0.975 Elapsed: 766.1s ETA: 13.1m\n",
            "[Epoch 1] Batch 2550/5072 Loss: 0.0665 Acc: 0.975 Elapsed: 780.3s ETA: 12.9m\n",
            "[Epoch 1] Batch 2600/5072 Loss: 0.0663 Acc: 0.975 Elapsed: 794.5s ETA: 12.6m\n",
            "[Epoch 1] Batch 2650/5072 Loss: 0.0662 Acc: 0.975 Elapsed: 808.8s ETA: 12.3m\n",
            "[Epoch 1] Batch 2700/5072 Loss: 0.0661 Acc: 0.975 Elapsed: 823.0s ETA: 12.1m\n",
            "[Epoch 1] Batch 2750/5072 Loss: 0.0659 Acc: 0.975 Elapsed: 837.2s ETA: 11.8m\n",
            "[Epoch 1] Batch 2800/5072 Loss: 0.0659 Acc: 0.975 Elapsed: 851.5s ETA: 11.5m\n",
            "[Epoch 1] Batch 2850/5072 Loss: 0.0657 Acc: 0.975 Elapsed: 865.7s ETA: 11.2m\n",
            "[Epoch 1] Batch 2900/5072 Loss: 0.0657 Acc: 0.975 Elapsed: 880.0s ETA: 11.0m\n",
            "[Epoch 1] Batch 2950/5072 Loss: 0.0657 Acc: 0.975 Elapsed: 894.2s ETA: 10.7m\n",
            "[Epoch 1] Batch 3000/5072 Loss: 0.0655 Acc: 0.975 Elapsed: 908.4s ETA: 10.5m\n",
            "[Epoch 1] Batch 3050/5072 Loss: 0.0655 Acc: 0.975 Elapsed: 922.7s ETA: 10.2m\n",
            "[Epoch 1] Batch 3100/5072 Loss: 0.0655 Acc: 0.975 Elapsed: 936.9s ETA: 9.9m\n",
            "[Epoch 1] Batch 3150/5072 Loss: 0.0654 Acc: 0.975 Elapsed: 951.2s ETA: 9.7m\n",
            "[Epoch 1] Batch 3200/5072 Loss: 0.0653 Acc: 0.975 Elapsed: 965.4s ETA: 9.4m\n",
            "[Epoch 1] Batch 3250/5072 Loss: 0.0653 Acc: 0.975 Elapsed: 979.6s ETA: 9.2m\n",
            "[Epoch 1] Batch 3300/5072 Loss: 0.0653 Acc: 0.975 Elapsed: 993.9s ETA: 8.9m\n",
            "[Epoch 1] Batch 3350/5072 Loss: 0.0651 Acc: 0.975 Elapsed: 1008.1s ETA: 8.6m\n",
            "[Epoch 1] Batch 3400/5072 Loss: 0.0650 Acc: 0.975 Elapsed: 1022.4s ETA: 8.4m\n",
            "[Epoch 1] Batch 3450/5072 Loss: 0.0650 Acc: 0.975 Elapsed: 1036.9s ETA: 8.1m\n",
            "[Epoch 1] Batch 3500/5072 Loss: 0.0649 Acc: 0.975 Elapsed: 1051.1s ETA: 7.9m\n",
            "[Epoch 1] Batch 3550/5072 Loss: 0.0649 Acc: 0.975 Elapsed: 1065.3s ETA: 7.6m\n",
            "[Epoch 1] Batch 3600/5072 Loss: 0.0648 Acc: 0.975 Elapsed: 1079.6s ETA: 7.4m\n",
            "[Epoch 1] Batch 3650/5072 Loss: 0.0646 Acc: 0.976 Elapsed: 1093.8s ETA: 7.1m\n",
            "[Epoch 1] Batch 3700/5072 Loss: 0.0645 Acc: 0.976 Elapsed: 1108.0s ETA: 6.8m\n",
            "[Epoch 1] Batch 3750/5072 Loss: 0.0645 Acc: 0.976 Elapsed: 1122.3s ETA: 6.6m\n",
            "[Epoch 1] Batch 3800/5072 Loss: 0.0643 Acc: 0.976 Elapsed: 1136.5s ETA: 6.3m\n",
            "[Epoch 1] Batch 3850/5072 Loss: 0.0643 Acc: 0.976 Elapsed: 1150.7s ETA: 6.1m\n",
            "[Epoch 1] Batch 3900/5072 Loss: 0.0642 Acc: 0.976 Elapsed: 1164.9s ETA: 5.8m\n",
            "[Epoch 1] Batch 3950/5072 Loss: 0.0641 Acc: 0.976 Elapsed: 1179.2s ETA: 5.6m\n",
            "[Epoch 1] Batch 4000/5072 Loss: 0.0641 Acc: 0.976 Elapsed: 1193.4s ETA: 5.3m\n",
            "[Epoch 1] Batch 4050/5072 Loss: 0.0640 Acc: 0.976 Elapsed: 1207.6s ETA: 5.1m\n",
            "[Epoch 1] Batch 4100/5072 Loss: 0.0638 Acc: 0.976 Elapsed: 1221.9s ETA: 4.8m\n",
            "[Epoch 1] Batch 4150/5072 Loss: 0.0638 Acc: 0.976 Elapsed: 1236.1s ETA: 4.6m\n",
            "[Epoch 1] Batch 4200/5072 Loss: 0.0637 Acc: 0.976 Elapsed: 1250.4s ETA: 4.3m\n",
            "[Epoch 1] Batch 4250/5072 Loss: 0.0637 Acc: 0.976 Elapsed: 1264.6s ETA: 4.1m\n",
            "[Epoch 1] Batch 4300/5072 Loss: 0.0636 Acc: 0.976 Elapsed: 1278.8s ETA: 3.8m\n",
            "[Epoch 1] Batch 4350/5072 Loss: 0.0636 Acc: 0.976 Elapsed: 1293.1s ETA: 3.6m\n",
            "[Epoch 1] Batch 4400/5072 Loss: 0.0635 Acc: 0.976 Elapsed: 1307.3s ETA: 3.3m\n",
            "[Epoch 1] Batch 4450/5072 Loss: 0.0634 Acc: 0.976 Elapsed: 1321.6s ETA: 3.1m\n",
            "[Epoch 1] Batch 4500/5072 Loss: 0.0634 Acc: 0.976 Elapsed: 1335.8s ETA: 2.8m\n",
            "[Epoch 1] Batch 4550/5072 Loss: 0.0633 Acc: 0.976 Elapsed: 1350.1s ETA: 2.6m\n",
            "[Epoch 1] Batch 4600/5072 Loss: 0.0632 Acc: 0.976 Elapsed: 1364.3s ETA: 2.3m\n",
            "[Epoch 1] Batch 4650/5072 Loss: 0.0632 Acc: 0.976 Elapsed: 1378.5s ETA: 2.1m\n",
            "[Epoch 1] Batch 4700/5072 Loss: 0.0631 Acc: 0.976 Elapsed: 1392.7s ETA: 1.8m\n",
            "[Epoch 1] Batch 4750/5072 Loss: 0.0630 Acc: 0.976 Elapsed: 1407.0s ETA: 1.6m\n",
            "[Epoch 1] Batch 4800/5072 Loss: 0.0630 Acc: 0.976 Elapsed: 1421.2s ETA: 1.3m\n",
            "[Epoch 1] Batch 4850/5072 Loss: 0.0629 Acc: 0.976 Elapsed: 1435.5s ETA: 1.1m\n",
            "[Epoch 1] Batch 4900/5072 Loss: 0.0629 Acc: 0.976 Elapsed: 1449.7s ETA: 0.8m\n",
            "[Epoch 1] Batch 4950/5072 Loss: 0.0629 Acc: 0.976 Elapsed: 1464.0s ETA: 0.6m\n",
            "[Epoch 1] Batch 5000/5072 Loss: 0.0628 Acc: 0.976 Elapsed: 1478.2s ETA: 0.4m\n",
            "[Epoch 1] Batch 5050/5072 Loss: 0.0627 Acc: 0.976 Elapsed: 1492.5s ETA: 0.1m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-8-3790742981.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/tmp/ipython-input-8-3790742981.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Epochs:  10%|█         | 1/10 [32:49<4:55:29, 1969.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 | Train: loss=0.0627, acc=0.976 | Val:   loss=0.0619, acc=0.977 | time=1962.0s (avg=1962.0s, ETA~294.3m) | ckpt→checkpoints/epoch_1.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-8-3790742981.py:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 2] Batch 50/5072 Loss: 0.0560 Acc: 0.978 Elapsed: 16.9s ETA: 28.2m\n",
            "[Epoch 2] Batch 100/5072 Loss: 0.0585 Acc: 0.977 Elapsed: 31.0s ETA: 25.7m\n",
            "[Epoch 2] Batch 150/5072 Loss: 0.0569 Acc: 0.978 Elapsed: 45.1s ETA: 24.7m\n",
            "[Epoch 2] Batch 200/5072 Loss: 0.0593 Acc: 0.977 Elapsed: 59.3s ETA: 24.1m\n",
            "[Epoch 2] Batch 250/5072 Loss: 0.0585 Acc: 0.977 Elapsed: 73.5s ETA: 23.6m\n",
            "[Epoch 2] Batch 300/5072 Loss: 0.0586 Acc: 0.977 Elapsed: 87.7s ETA: 23.2m\n",
            "[Epoch 2] Batch 350/5072 Loss: 0.0574 Acc: 0.978 Elapsed: 101.9s ETA: 22.9m\n",
            "[Epoch 2] Batch 400/5072 Loss: 0.0566 Acc: 0.978 Elapsed: 116.1s ETA: 22.6m\n",
            "[Epoch 2] Batch 450/5072 Loss: 0.0564 Acc: 0.978 Elapsed: 130.3s ETA: 22.3m\n",
            "[Epoch 2] Batch 500/5072 Loss: 0.0559 Acc: 0.978 Elapsed: 144.5s ETA: 22.0m\n",
            "[Epoch 2] Batch 550/5072 Loss: 0.0558 Acc: 0.978 Elapsed: 158.7s ETA: 21.8m\n",
            "[Epoch 2] Batch 600/5072 Loss: 0.0555 Acc: 0.978 Elapsed: 173.0s ETA: 21.5m\n",
            "[Epoch 2] Batch 650/5072 Loss: 0.0557 Acc: 0.978 Elapsed: 187.2s ETA: 21.2m\n",
            "[Epoch 2] Batch 700/5072 Loss: 0.0559 Acc: 0.978 Elapsed: 201.4s ETA: 21.0m\n",
            "[Epoch 2] Batch 750/5072 Loss: 0.0559 Acc: 0.978 Elapsed: 215.6s ETA: 20.7m\n",
            "[Epoch 2] Batch 800/5072 Loss: 0.0560 Acc: 0.978 Elapsed: 229.8s ETA: 20.5m\n",
            "[Epoch 2] Batch 850/5072 Loss: 0.0557 Acc: 0.978 Elapsed: 244.0s ETA: 20.2m\n",
            "[Epoch 2] Batch 900/5072 Loss: 0.0558 Acc: 0.978 Elapsed: 258.2s ETA: 20.0m\n",
            "[Epoch 2] Batch 950/5072 Loss: 0.0557 Acc: 0.978 Elapsed: 272.4s ETA: 19.7m\n",
            "[Epoch 2] Batch 1000/5072 Loss: 0.0554 Acc: 0.978 Elapsed: 286.7s ETA: 19.5m\n",
            "[Epoch 2] Batch 1050/5072 Loss: 0.0549 Acc: 0.978 Elapsed: 300.9s ETA: 19.2m\n",
            "[Epoch 2] Batch 1100/5072 Loss: 0.0549 Acc: 0.978 Elapsed: 315.1s ETA: 19.0m\n",
            "[Epoch 2] Batch 1150/5072 Loss: 0.0548 Acc: 0.979 Elapsed: 329.3s ETA: 18.7m\n",
            "[Epoch 2] Batch 1200/5072 Loss: 0.0550 Acc: 0.978 Elapsed: 343.6s ETA: 18.5m\n",
            "[Epoch 2] Batch 1250/5072 Loss: 0.0553 Acc: 0.978 Elapsed: 357.8s ETA: 18.2m\n",
            "[Epoch 2] Batch 1300/5072 Loss: 0.0553 Acc: 0.978 Elapsed: 372.1s ETA: 18.0m\n",
            "[Epoch 2] Batch 1350/5072 Loss: 0.0554 Acc: 0.978 Elapsed: 386.3s ETA: 17.8m\n",
            "[Epoch 2] Batch 1400/5072 Loss: 0.0552 Acc: 0.978 Elapsed: 400.5s ETA: 17.5m\n",
            "[Epoch 2] Batch 1450/5072 Loss: 0.0554 Acc: 0.978 Elapsed: 414.7s ETA: 17.3m\n",
            "[Epoch 2] Batch 1500/5072 Loss: 0.0554 Acc: 0.978 Elapsed: 429.0s ETA: 17.0m\n",
            "[Epoch 2] Batch 1550/5072 Loss: 0.0555 Acc: 0.978 Elapsed: 443.2s ETA: 16.8m\n",
            "[Epoch 2] Batch 1600/5072 Loss: 0.0555 Acc: 0.978 Elapsed: 457.5s ETA: 16.5m\n",
            "[Epoch 2] Batch 1650/5072 Loss: 0.0553 Acc: 0.978 Elapsed: 471.7s ETA: 16.3m\n",
            "[Epoch 2] Batch 1700/5072 Loss: 0.0551 Acc: 0.978 Elapsed: 485.9s ETA: 16.1m\n",
            "[Epoch 2] Batch 1750/5072 Loss: 0.0549 Acc: 0.978 Elapsed: 500.1s ETA: 15.8m\n",
            "[Epoch 2] Batch 1800/5072 Loss: 0.0549 Acc: 0.978 Elapsed: 514.4s ETA: 15.6m\n",
            "[Epoch 2] Batch 1850/5072 Loss: 0.0550 Acc: 0.978 Elapsed: 528.6s ETA: 15.3m\n",
            "[Epoch 2] Batch 1900/5072 Loss: 0.0551 Acc: 0.978 Elapsed: 542.8s ETA: 15.1m\n",
            "[Epoch 2] Batch 1950/5072 Loss: 0.0552 Acc: 0.978 Elapsed: 557.1s ETA: 14.9m\n",
            "[Epoch 2] Batch 2000/5072 Loss: 0.0552 Acc: 0.978 Elapsed: 571.3s ETA: 14.6m\n",
            "[Epoch 2] Batch 2050/5072 Loss: 0.0553 Acc: 0.978 Elapsed: 585.5s ETA: 14.4m\n",
            "[Epoch 2] Batch 2100/5072 Loss: 0.0554 Acc: 0.978 Elapsed: 599.7s ETA: 14.1m\n",
            "[Epoch 2] Batch 2150/5072 Loss: 0.0554 Acc: 0.978 Elapsed: 614.0s ETA: 13.9m\n",
            "[Epoch 2] Batch 2200/5072 Loss: 0.0553 Acc: 0.978 Elapsed: 628.2s ETA: 13.7m\n",
            "[Epoch 2] Batch 2250/5072 Loss: 0.0552 Acc: 0.978 Elapsed: 642.4s ETA: 13.4m\n",
            "[Epoch 2] Batch 2300/5072 Loss: 0.0552 Acc: 0.978 Elapsed: 656.7s ETA: 13.2m\n",
            "[Epoch 2] Batch 2350/5072 Loss: 0.0551 Acc: 0.978 Elapsed: 670.9s ETA: 13.0m\n",
            "[Epoch 2] Batch 2400/5072 Loss: 0.0552 Acc: 0.978 Elapsed: 685.2s ETA: 12.7m\n",
            "[Epoch 2] Batch 2450/5072 Loss: 0.0552 Acc: 0.978 Elapsed: 699.4s ETA: 12.5m\n",
            "[Epoch 2] Batch 2500/5072 Loss: 0.0552 Acc: 0.978 Elapsed: 713.6s ETA: 12.2m\n",
            "[Epoch 2] Batch 2550/5072 Loss: 0.0552 Acc: 0.978 Elapsed: 727.8s ETA: 12.0m\n",
            "[Epoch 2] Batch 2600/5072 Loss: 0.0551 Acc: 0.978 Elapsed: 742.0s ETA: 11.8m\n",
            "[Epoch 2] Batch 2650/5072 Loss: 0.0551 Acc: 0.978 Elapsed: 756.3s ETA: 11.5m\n",
            "[Epoch 2] Batch 2700/5072 Loss: 0.0551 Acc: 0.978 Elapsed: 770.5s ETA: 11.3m\n",
            "[Epoch 2] Batch 2750/5072 Loss: 0.0551 Acc: 0.978 Elapsed: 784.7s ETA: 11.0m\n",
            "[Epoch 2] Batch 2800/5072 Loss: 0.0550 Acc: 0.978 Elapsed: 799.0s ETA: 10.8m\n",
            "[Epoch 2] Batch 2850/5072 Loss: 0.0550 Acc: 0.978 Elapsed: 813.2s ETA: 10.6m\n",
            "[Epoch 2] Batch 2900/5072 Loss: 0.0551 Acc: 0.978 Elapsed: 827.4s ETA: 10.3m\n",
            "[Epoch 2] Batch 2950/5072 Loss: 0.0551 Acc: 0.978 Elapsed: 841.6s ETA: 10.1m\n",
            "[Epoch 2] Batch 3000/5072 Loss: 0.0550 Acc: 0.978 Elapsed: 855.9s ETA: 9.9m\n",
            "[Epoch 2] Batch 3050/5072 Loss: 0.0549 Acc: 0.978 Elapsed: 870.1s ETA: 9.6m\n",
            "[Epoch 2] Batch 3100/5072 Loss: 0.0549 Acc: 0.978 Elapsed: 884.3s ETA: 9.4m\n",
            "[Epoch 2] Batch 3150/5072 Loss: 0.0548 Acc: 0.978 Elapsed: 898.5s ETA: 9.1m\n",
            "[Epoch 2] Batch 3200/5072 Loss: 0.0548 Acc: 0.978 Elapsed: 912.7s ETA: 8.9m\n",
            "[Epoch 2] Batch 3250/5072 Loss: 0.0547 Acc: 0.978 Elapsed: 927.0s ETA: 8.7m\n",
            "[Epoch 2] Batch 3300/5072 Loss: 0.0548 Acc: 0.978 Elapsed: 941.2s ETA: 8.4m\n",
            "[Epoch 2] Batch 3350/5072 Loss: 0.0547 Acc: 0.978 Elapsed: 955.5s ETA: 8.2m\n",
            "[Epoch 2] Batch 3400/5072 Loss: 0.0546 Acc: 0.979 Elapsed: 969.7s ETA: 7.9m\n",
            "[Epoch 2] Batch 3450/5072 Loss: 0.0547 Acc: 0.978 Elapsed: 983.9s ETA: 7.7m\n",
            "[Epoch 2] Batch 3500/5072 Loss: 0.0546 Acc: 0.979 Elapsed: 998.2s ETA: 7.5m\n",
            "[Epoch 2] Batch 3550/5072 Loss: 0.0545 Acc: 0.979 Elapsed: 1012.4s ETA: 7.2m\n",
            "[Epoch 2] Batch 3600/5072 Loss: 0.0545 Acc: 0.979 Elapsed: 1026.7s ETA: 7.0m\n",
            "[Epoch 2] Batch 3650/5072 Loss: 0.0544 Acc: 0.979 Elapsed: 1040.9s ETA: 6.8m\n",
            "[Epoch 2] Batch 3700/5072 Loss: 0.0545 Acc: 0.979 Elapsed: 1055.2s ETA: 6.5m\n",
            "[Epoch 2] Batch 3750/5072 Loss: 0.0545 Acc: 0.979 Elapsed: 1069.4s ETA: 6.3m\n",
            "[Epoch 2] Batch 3800/5072 Loss: 0.0545 Acc: 0.979 Elapsed: 1083.6s ETA: 6.0m\n",
            "[Epoch 2] Batch 3850/5072 Loss: 0.0545 Acc: 0.979 Elapsed: 1097.9s ETA: 5.8m\n",
            "[Epoch 2] Batch 3900/5072 Loss: 0.0545 Acc: 0.979 Elapsed: 1112.1s ETA: 5.6m\n",
            "[Epoch 2] Batch 3950/5072 Loss: 0.0545 Acc: 0.979 Elapsed: 1126.3s ETA: 5.3m\n",
            "[Epoch 2] Batch 4000/5072 Loss: 0.0544 Acc: 0.979 Elapsed: 1140.6s ETA: 5.1m\n",
            "[Epoch 2] Batch 4050/5072 Loss: 0.0545 Acc: 0.979 Elapsed: 1154.8s ETA: 4.9m\n",
            "[Epoch 2] Batch 4100/5072 Loss: 0.0545 Acc: 0.979 Elapsed: 1169.1s ETA: 4.6m\n",
            "[Epoch 2] Batch 4150/5072 Loss: 0.0545 Acc: 0.979 Elapsed: 1183.3s ETA: 4.4m\n",
            "[Epoch 2] Batch 4200/5072 Loss: 0.0545 Acc: 0.979 Elapsed: 1197.5s ETA: 4.1m\n",
            "[Epoch 2] Batch 4250/5072 Loss: 0.0544 Acc: 0.979 Elapsed: 1211.8s ETA: 3.9m\n",
            "[Epoch 2] Batch 4300/5072 Loss: 0.0543 Acc: 0.979 Elapsed: 1226.0s ETA: 3.7m\n",
            "[Epoch 2] Batch 4350/5072 Loss: 0.0542 Acc: 0.979 Elapsed: 1240.2s ETA: 3.4m\n",
            "[Epoch 2] Batch 4400/5072 Loss: 0.0542 Acc: 0.979 Elapsed: 1254.4s ETA: 3.2m\n",
            "[Epoch 2] Batch 4450/5072 Loss: 0.0541 Acc: 0.979 Elapsed: 1268.6s ETA: 3.0m\n",
            "[Epoch 2] Batch 4500/5072 Loss: 0.0541 Acc: 0.979 Elapsed: 1282.9s ETA: 2.7m\n",
            "[Epoch 2] Batch 4550/5072 Loss: 0.0541 Acc: 0.979 Elapsed: 1297.1s ETA: 2.5m\n",
            "[Epoch 2] Batch 4600/5072 Loss: 0.0541 Acc: 0.979 Elapsed: 1311.4s ETA: 2.2m\n",
            "[Epoch 2] Batch 4650/5072 Loss: 0.0542 Acc: 0.979 Elapsed: 1325.6s ETA: 2.0m\n",
            "[Epoch 2] Batch 4700/5072 Loss: 0.0541 Acc: 0.979 Elapsed: 1339.8s ETA: 1.8m\n",
            "[Epoch 2] Batch 4750/5072 Loss: 0.0541 Acc: 0.979 Elapsed: 1354.1s ETA: 1.5m\n",
            "[Epoch 2] Batch 4800/5072 Loss: 0.0541 Acc: 0.979 Elapsed: 1368.3s ETA: 1.3m\n",
            "[Epoch 2] Batch 4850/5072 Loss: 0.0541 Acc: 0.979 Elapsed: 1382.6s ETA: 1.1m\n",
            "[Epoch 2] Batch 4900/5072 Loss: 0.0541 Acc: 0.979 Elapsed: 1396.8s ETA: 0.8m\n",
            "[Epoch 2] Batch 4950/5072 Loss: 0.0541 Acc: 0.979 Elapsed: 1411.0s ETA: 0.6m\n",
            "[Epoch 2] Batch 5000/5072 Loss: 0.0542 Acc: 0.979 Elapsed: 1425.3s ETA: 0.3m\n",
            "[Epoch 2] Batch 5050/5072 Loss: 0.0542 Acc: 0.979 Elapsed: 1439.6s ETA: 0.1m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  20%|██        | 2/10 [59:06<3:51:46, 1738.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 | Train: loss=0.0542, acc=0.979 | Val:   loss=0.0586, acc=0.978 | time=1567.7s (avg=1764.8s, ETA~235.3m) | ckpt→checkpoints/epoch_2.pth\n",
            "[Epoch 3] Batch 50/5072 Loss: 0.0494 Acc: 0.982 Elapsed: 16.3s ETA: 27.2m\n",
            "[Epoch 3] Batch 100/5072 Loss: 0.0523 Acc: 0.980 Elapsed: 30.4s ETA: 25.2m\n",
            "[Epoch 3] Batch 150/5072 Loss: 0.0503 Acc: 0.981 Elapsed: 44.6s ETA: 24.4m\n",
            "[Epoch 3] Batch 200/5072 Loss: 0.0507 Acc: 0.980 Elapsed: 58.8s ETA: 23.9m\n",
            "[Epoch 3] Batch 250/5072 Loss: 0.0519 Acc: 0.979 Elapsed: 73.0s ETA: 23.5m\n",
            "[Epoch 3] Batch 300/5072 Loss: 0.0536 Acc: 0.979 Elapsed: 87.2s ETA: 23.1m\n",
            "[Epoch 3] Batch 350/5072 Loss: 0.0522 Acc: 0.979 Elapsed: 101.5s ETA: 22.8m\n",
            "[Epoch 3] Batch 400/5072 Loss: 0.0520 Acc: 0.979 Elapsed: 115.7s ETA: 22.5m\n",
            "[Epoch 3] Batch 450/5072 Loss: 0.0516 Acc: 0.979 Elapsed: 130.0s ETA: 22.2m\n",
            "[Epoch 3] Batch 500/5072 Loss: 0.0515 Acc: 0.979 Elapsed: 144.2s ETA: 22.0m\n",
            "[Epoch 3] Batch 550/5072 Loss: 0.0515 Acc: 0.979 Elapsed: 158.5s ETA: 21.7m\n",
            "[Epoch 3] Batch 600/5072 Loss: 0.0516 Acc: 0.979 Elapsed: 172.7s ETA: 21.5m\n",
            "[Epoch 3] Batch 650/5072 Loss: 0.0520 Acc: 0.979 Elapsed: 186.9s ETA: 21.2m\n",
            "[Epoch 3] Batch 700/5072 Loss: 0.0527 Acc: 0.979 Elapsed: 201.2s ETA: 20.9m\n",
            "[Epoch 3] Batch 750/5072 Loss: 0.0523 Acc: 0.979 Elapsed: 215.4s ETA: 20.7m\n",
            "[Epoch 3] Batch 800/5072 Loss: 0.0521 Acc: 0.979 Elapsed: 229.7s ETA: 20.4m\n",
            "[Epoch 3] Batch 850/5072 Loss: 0.0520 Acc: 0.979 Elapsed: 243.9s ETA: 20.2m\n",
            "[Epoch 3] Batch 900/5072 Loss: 0.0523 Acc: 0.979 Elapsed: 258.2s ETA: 19.9m\n",
            "[Epoch 3] Batch 950/5072 Loss: 0.0524 Acc: 0.979 Elapsed: 272.4s ETA: 19.7m\n",
            "[Epoch 3] Batch 1000/5072 Loss: 0.0520 Acc: 0.979 Elapsed: 286.7s ETA: 19.5m\n",
            "[Epoch 3] Batch 1050/5072 Loss: 0.0515 Acc: 0.979 Elapsed: 300.9s ETA: 19.2m\n",
            "[Epoch 3] Batch 1100/5072 Loss: 0.0517 Acc: 0.979 Elapsed: 315.1s ETA: 19.0m\n",
            "[Epoch 3] Batch 1150/5072 Loss: 0.0518 Acc: 0.979 Elapsed: 329.4s ETA: 18.7m\n",
            "[Epoch 3] Batch 1200/5072 Loss: 0.0517 Acc: 0.979 Elapsed: 343.6s ETA: 18.5m\n",
            "[Epoch 3] Batch 1250/5072 Loss: 0.0515 Acc: 0.979 Elapsed: 357.8s ETA: 18.2m\n",
            "[Epoch 3] Batch 1300/5072 Loss: 0.0515 Acc: 0.979 Elapsed: 372.1s ETA: 18.0m\n",
            "[Epoch 3] Batch 1350/5072 Loss: 0.0515 Acc: 0.979 Elapsed: 386.3s ETA: 17.8m\n",
            "[Epoch 3] Batch 1400/5072 Loss: 0.0514 Acc: 0.979 Elapsed: 400.5s ETA: 17.5m\n",
            "[Epoch 3] Batch 1450/5072 Loss: 0.0514 Acc: 0.979 Elapsed: 414.8s ETA: 17.3m\n",
            "[Epoch 3] Batch 1500/5072 Loss: 0.0514 Acc: 0.979 Elapsed: 429.0s ETA: 17.0m\n",
            "[Epoch 3] Batch 1550/5072 Loss: 0.0514 Acc: 0.979 Elapsed: 443.3s ETA: 16.8m\n",
            "[Epoch 3] Batch 1600/5072 Loss: 0.0512 Acc: 0.979 Elapsed: 457.5s ETA: 16.5m\n",
            "[Epoch 3] Batch 1650/5072 Loss: 0.0511 Acc: 0.979 Elapsed: 471.7s ETA: 16.3m\n",
            "[Epoch 3] Batch 1700/5072 Loss: 0.0511 Acc: 0.979 Elapsed: 486.0s ETA: 16.1m\n",
            "[Epoch 3] Batch 1750/5072 Loss: 0.0513 Acc: 0.979 Elapsed: 500.2s ETA: 15.8m\n",
            "[Epoch 3] Batch 1800/5072 Loss: 0.0513 Acc: 0.979 Elapsed: 514.5s ETA: 15.6m\n",
            "[Epoch 3] Batch 1850/5072 Loss: 0.0512 Acc: 0.979 Elapsed: 528.7s ETA: 15.3m\n",
            "[Epoch 3] Batch 1900/5072 Loss: 0.0513 Acc: 0.979 Elapsed: 543.0s ETA: 15.1m\n",
            "[Epoch 3] Batch 1950/5072 Loss: 0.0512 Acc: 0.979 Elapsed: 557.2s ETA: 14.9m\n",
            "[Epoch 3] Batch 2000/5072 Loss: 0.0511 Acc: 0.979 Elapsed: 571.5s ETA: 14.6m\n",
            "[Epoch 3] Batch 2050/5072 Loss: 0.0510 Acc: 0.980 Elapsed: 587.1s ETA: 14.4m\n",
            "[Epoch 3] Batch 2100/5072 Loss: 0.0511 Acc: 0.979 Elapsed: 601.3s ETA: 14.2m\n",
            "[Epoch 3] Batch 2150/5072 Loss: 0.0510 Acc: 0.980 Elapsed: 615.5s ETA: 13.9m\n",
            "[Epoch 3] Batch 2200/5072 Loss: 0.0510 Acc: 0.980 Elapsed: 629.7s ETA: 13.7m\n",
            "[Epoch 3] Batch 2250/5072 Loss: 0.0510 Acc: 0.980 Elapsed: 643.8s ETA: 13.5m\n",
            "[Epoch 3] Batch 2300/5072 Loss: 0.0509 Acc: 0.980 Elapsed: 658.0s ETA: 13.2m\n",
            "[Epoch 3] Batch 2350/5072 Loss: 0.0509 Acc: 0.980 Elapsed: 672.2s ETA: 13.0m\n",
            "[Epoch 3] Batch 2400/5072 Loss: 0.0507 Acc: 0.980 Elapsed: 686.4s ETA: 12.7m\n",
            "[Epoch 3] Batch 2450/5072 Loss: 0.0507 Acc: 0.980 Elapsed: 700.6s ETA: 12.5m\n",
            "[Epoch 3] Batch 2500/5072 Loss: 0.0508 Acc: 0.980 Elapsed: 714.8s ETA: 12.3m\n",
            "[Epoch 3] Batch 2550/5072 Loss: 0.0508 Acc: 0.980 Elapsed: 729.0s ETA: 12.0m\n",
            "[Epoch 3] Batch 2600/5072 Loss: 0.0507 Acc: 0.980 Elapsed: 743.2s ETA: 11.8m\n",
            "[Epoch 3] Batch 2650/5072 Loss: 0.0507 Acc: 0.980 Elapsed: 757.4s ETA: 11.5m\n",
            "[Epoch 3] Batch 2700/5072 Loss: 0.0506 Acc: 0.980 Elapsed: 771.6s ETA: 11.3m\n",
            "[Epoch 3] Batch 2750/5072 Loss: 0.0506 Acc: 0.980 Elapsed: 785.9s ETA: 11.1m\n",
            "[Epoch 3] Batch 2800/5072 Loss: 0.0505 Acc: 0.980 Elapsed: 800.1s ETA: 10.8m\n",
            "[Epoch 3] Batch 2850/5072 Loss: 0.0507 Acc: 0.980 Elapsed: 814.2s ETA: 10.6m\n",
            "[Epoch 3] Batch 2900/5072 Loss: 0.0507 Acc: 0.980 Elapsed: 828.4s ETA: 10.3m\n",
            "[Epoch 3] Batch 2950/5072 Loss: 0.0506 Acc: 0.980 Elapsed: 842.6s ETA: 10.1m\n",
            "[Epoch 3] Batch 3000/5072 Loss: 0.0506 Acc: 0.980 Elapsed: 856.8s ETA: 9.9m\n",
            "[Epoch 3] Batch 3050/5072 Loss: 0.0506 Acc: 0.980 Elapsed: 871.0s ETA: 9.6m\n",
            "[Epoch 3] Batch 3100/5072 Loss: 0.0507 Acc: 0.980 Elapsed: 885.3s ETA: 9.4m\n",
            "[Epoch 3] Batch 3150/5072 Loss: 0.0507 Acc: 0.980 Elapsed: 899.6s ETA: 9.1m\n",
            "[Epoch 3] Batch 3200/5072 Loss: 0.0507 Acc: 0.980 Elapsed: 913.9s ETA: 8.9m\n",
            "[Epoch 3] Batch 3250/5072 Loss: 0.0506 Acc: 0.980 Elapsed: 928.2s ETA: 8.7m\n",
            "[Epoch 3] Batch 3300/5072 Loss: 0.0507 Acc: 0.980 Elapsed: 942.5s ETA: 8.4m\n",
            "[Epoch 3] Batch 3350/5072 Loss: 0.0506 Acc: 0.980 Elapsed: 956.8s ETA: 8.2m\n",
            "[Epoch 3] Batch 3400/5072 Loss: 0.0505 Acc: 0.980 Elapsed: 971.0s ETA: 8.0m\n",
            "[Epoch 3] Batch 3450/5072 Loss: 0.0506 Acc: 0.980 Elapsed: 985.4s ETA: 7.7m\n",
            "[Epoch 3] Batch 3500/5072 Loss: 0.0506 Acc: 0.980 Elapsed: 999.6s ETA: 7.5m\n",
            "[Epoch 3] Batch 3550/5072 Loss: 0.0506 Acc: 0.980 Elapsed: 1013.9s ETA: 7.2m\n",
            "[Epoch 3] Batch 3600/5072 Loss: 0.0505 Acc: 0.980 Elapsed: 1028.2s ETA: 7.0m\n",
            "[Epoch 3] Batch 3650/5072 Loss: 0.0505 Acc: 0.980 Elapsed: 1042.4s ETA: 6.8m\n",
            "[Epoch 3] Batch 3700/5072 Loss: 0.0505 Acc: 0.980 Elapsed: 1056.7s ETA: 6.5m\n",
            "[Epoch 3] Batch 3750/5072 Loss: 0.0505 Acc: 0.980 Elapsed: 1071.0s ETA: 6.3m\n",
            "[Epoch 3] Batch 3800/5072 Loss: 0.0504 Acc: 0.980 Elapsed: 1085.2s ETA: 6.1m\n",
            "[Epoch 3] Batch 3850/5072 Loss: 0.0503 Acc: 0.980 Elapsed: 1099.5s ETA: 5.8m\n",
            "[Epoch 3] Batch 3900/5072 Loss: 0.0504 Acc: 0.980 Elapsed: 1113.8s ETA: 5.6m\n",
            "[Epoch 3] Batch 3950/5072 Loss: 0.0504 Acc: 0.980 Elapsed: 1128.1s ETA: 5.3m\n",
            "[Epoch 3] Batch 4000/5072 Loss: 0.0504 Acc: 0.980 Elapsed: 1142.3s ETA: 5.1m\n",
            "[Epoch 3] Batch 4050/5072 Loss: 0.0504 Acc: 0.980 Elapsed: 1156.6s ETA: 4.9m\n",
            "[Epoch 3] Batch 4100/5072 Loss: 0.0504 Acc: 0.980 Elapsed: 1170.9s ETA: 4.6m\n",
            "[Epoch 3] Batch 4150/5072 Loss: 0.0504 Acc: 0.980 Elapsed: 1185.1s ETA: 4.4m\n",
            "[Epoch 3] Batch 4200/5072 Loss: 0.0503 Acc: 0.980 Elapsed: 1199.4s ETA: 4.2m\n",
            "[Epoch 3] Batch 4250/5072 Loss: 0.0504 Acc: 0.980 Elapsed: 1213.8s ETA: 3.9m\n",
            "[Epoch 3] Batch 4300/5072 Loss: 0.0504 Acc: 0.980 Elapsed: 1228.1s ETA: 3.7m\n",
            "[Epoch 3] Batch 4350/5072 Loss: 0.0502 Acc: 0.980 Elapsed: 1242.4s ETA: 3.4m\n",
            "[Epoch 3] Batch 4400/5072 Loss: 0.0502 Acc: 0.980 Elapsed: 1256.7s ETA: 3.2m\n",
            "[Epoch 3] Batch 4450/5072 Loss: 0.0502 Acc: 0.980 Elapsed: 1271.0s ETA: 3.0m\n",
            "[Epoch 3] Batch 4500/5072 Loss: 0.0502 Acc: 0.980 Elapsed: 1285.3s ETA: 2.7m\n",
            "[Epoch 3] Batch 4550/5072 Loss: 0.0502 Acc: 0.980 Elapsed: 1299.6s ETA: 2.5m\n",
            "[Epoch 3] Batch 4600/5072 Loss: 0.0501 Acc: 0.980 Elapsed: 1313.9s ETA: 2.2m\n",
            "[Epoch 3] Batch 4650/5072 Loss: 0.0502 Acc: 0.980 Elapsed: 1328.2s ETA: 2.0m\n",
            "[Epoch 3] Batch 4700/5072 Loss: 0.0502 Acc: 0.980 Elapsed: 1342.5s ETA: 1.8m\n",
            "[Epoch 3] Batch 4750/5072 Loss: 0.0502 Acc: 0.980 Elapsed: 1356.8s ETA: 1.5m\n",
            "[Epoch 3] Batch 4800/5072 Loss: 0.0502 Acc: 0.980 Elapsed: 1371.1s ETA: 1.3m\n",
            "[Epoch 3] Batch 4850/5072 Loss: 0.0503 Acc: 0.980 Elapsed: 1385.4s ETA: 1.1m\n",
            "[Epoch 3] Batch 4900/5072 Loss: 0.0502 Acc: 0.980 Elapsed: 1399.8s ETA: 0.8m\n",
            "[Epoch 3] Batch 4950/5072 Loss: 0.0503 Acc: 0.980 Elapsed: 1414.1s ETA: 0.6m\n",
            "[Epoch 3] Batch 5000/5072 Loss: 0.0503 Acc: 0.980 Elapsed: 1428.4s ETA: 0.3m\n",
            "[Epoch 3] Batch 5050/5072 Loss: 0.0503 Acc: 0.980 Elapsed: 1442.7s ETA: 0.1m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  30%|███       | 3/10 [1:25:27<3:14:26, 1666.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 | Train: loss=0.0503, acc=0.980 | Val:   loss=0.0545, acc=0.979 | time=1572.7s (avg=1700.8s, ETA~198.4m) | ckpt→checkpoints/epoch_3.pth\n",
            "[Epoch 4] Batch 50/5072 Loss: 0.0496 Acc: 0.981 Elapsed: 16.7s ETA: 27.9m\n",
            "[Epoch 4] Batch 100/5072 Loss: 0.0472 Acc: 0.982 Elapsed: 30.9s ETA: 25.6m\n",
            "[Epoch 4] Batch 150/5072 Loss: 0.0463 Acc: 0.982 Elapsed: 45.2s ETA: 24.7m\n",
            "[Epoch 4] Batch 200/5072 Loss: 0.0467 Acc: 0.981 Elapsed: 59.5s ETA: 24.2m\n",
            "[Epoch 4] Batch 250/5072 Loss: 0.0484 Acc: 0.981 Elapsed: 73.8s ETA: 23.7m\n",
            "[Epoch 4] Batch 300/5072 Loss: 0.0484 Acc: 0.980 Elapsed: 88.0s ETA: 23.3m\n",
            "[Epoch 4] Batch 350/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 102.4s ETA: 23.0m\n",
            "[Epoch 4] Batch 400/5072 Loss: 0.0467 Acc: 0.981 Elapsed: 116.6s ETA: 22.7m\n",
            "[Epoch 4] Batch 450/5072 Loss: 0.0470 Acc: 0.981 Elapsed: 130.9s ETA: 22.4m\n",
            "[Epoch 4] Batch 500/5072 Loss: 0.0473 Acc: 0.981 Elapsed: 145.2s ETA: 22.1m\n",
            "[Epoch 4] Batch 550/5072 Loss: 0.0469 Acc: 0.981 Elapsed: 159.5s ETA: 21.9m\n",
            "[Epoch 4] Batch 600/5072 Loss: 0.0465 Acc: 0.982 Elapsed: 173.8s ETA: 21.6m\n",
            "[Epoch 4] Batch 650/5072 Loss: 0.0466 Acc: 0.981 Elapsed: 188.1s ETA: 21.3m\n",
            "[Epoch 4] Batch 700/5072 Loss: 0.0463 Acc: 0.981 Elapsed: 202.4s ETA: 21.1m\n",
            "[Epoch 4] Batch 750/5072 Loss: 0.0467 Acc: 0.981 Elapsed: 216.7s ETA: 20.8m\n",
            "[Epoch 4] Batch 800/5072 Loss: 0.0466 Acc: 0.981 Elapsed: 231.0s ETA: 20.6m\n",
            "[Epoch 4] Batch 850/5072 Loss: 0.0463 Acc: 0.981 Elapsed: 245.3s ETA: 20.3m\n",
            "[Epoch 4] Batch 900/5072 Loss: 0.0464 Acc: 0.981 Elapsed: 259.6s ETA: 20.1m\n",
            "[Epoch 4] Batch 950/5072 Loss: 0.0467 Acc: 0.981 Elapsed: 273.9s ETA: 19.8m\n",
            "[Epoch 4] Batch 1000/5072 Loss: 0.0466 Acc: 0.981 Elapsed: 288.3s ETA: 19.6m\n",
            "[Epoch 4] Batch 1050/5072 Loss: 0.0467 Acc: 0.981 Elapsed: 302.6s ETA: 19.3m\n",
            "[Epoch 4] Batch 1100/5072 Loss: 0.0469 Acc: 0.981 Elapsed: 316.9s ETA: 19.1m\n",
            "[Epoch 4] Batch 1150/5072 Loss: 0.0468 Acc: 0.981 Elapsed: 331.3s ETA: 18.8m\n",
            "[Epoch 4] Batch 1200/5072 Loss: 0.0468 Acc: 0.981 Elapsed: 345.6s ETA: 18.6m\n",
            "[Epoch 4] Batch 1250/5072 Loss: 0.0470 Acc: 0.981 Elapsed: 359.9s ETA: 18.3m\n",
            "[Epoch 4] Batch 1300/5072 Loss: 0.0470 Acc: 0.981 Elapsed: 374.2s ETA: 18.1m\n",
            "[Epoch 4] Batch 1350/5072 Loss: 0.0472 Acc: 0.981 Elapsed: 388.6s ETA: 17.9m\n",
            "[Epoch 4] Batch 1400/5072 Loss: 0.0470 Acc: 0.981 Elapsed: 402.8s ETA: 17.6m\n",
            "[Epoch 4] Batch 1450/5072 Loss: 0.0469 Acc: 0.981 Elapsed: 417.2s ETA: 17.4m\n",
            "[Epoch 4] Batch 1500/5072 Loss: 0.0470 Acc: 0.981 Elapsed: 431.5s ETA: 17.1m\n",
            "[Epoch 4] Batch 1550/5072 Loss: 0.0470 Acc: 0.981 Elapsed: 445.8s ETA: 16.9m\n",
            "[Epoch 4] Batch 1600/5072 Loss: 0.0470 Acc: 0.981 Elapsed: 460.1s ETA: 16.6m\n",
            "[Epoch 4] Batch 1650/5072 Loss: 0.0467 Acc: 0.982 Elapsed: 474.4s ETA: 16.4m\n",
            "[Epoch 4] Batch 1700/5072 Loss: 0.0469 Acc: 0.981 Elapsed: 488.8s ETA: 16.2m\n",
            "[Epoch 4] Batch 1750/5072 Loss: 0.0471 Acc: 0.981 Elapsed: 503.1s ETA: 15.9m\n",
            "[Epoch 4] Batch 1800/5072 Loss: 0.0470 Acc: 0.981 Elapsed: 517.4s ETA: 15.7m\n",
            "[Epoch 4] Batch 1850/5072 Loss: 0.0469 Acc: 0.981 Elapsed: 531.7s ETA: 15.4m\n",
            "[Epoch 4] Batch 1900/5072 Loss: 0.0471 Acc: 0.981 Elapsed: 546.0s ETA: 15.2m\n",
            "[Epoch 4] Batch 1950/5072 Loss: 0.0471 Acc: 0.981 Elapsed: 560.4s ETA: 15.0m\n",
            "[Epoch 4] Batch 2000/5072 Loss: 0.0470 Acc: 0.981 Elapsed: 574.7s ETA: 14.7m\n",
            "[Epoch 4] Batch 2050/5072 Loss: 0.0470 Acc: 0.981 Elapsed: 589.0s ETA: 14.5m\n",
            "[Epoch 4] Batch 2100/5072 Loss: 0.0472 Acc: 0.981 Elapsed: 603.3s ETA: 14.2m\n",
            "[Epoch 4] Batch 2150/5072 Loss: 0.0472 Acc: 0.981 Elapsed: 617.7s ETA: 14.0m\n",
            "[Epoch 4] Batch 2200/5072 Loss: 0.0472 Acc: 0.981 Elapsed: 631.9s ETA: 13.7m\n",
            "[Epoch 4] Batch 2250/5072 Loss: 0.0473 Acc: 0.981 Elapsed: 646.3s ETA: 13.5m\n",
            "[Epoch 4] Batch 2300/5072 Loss: 0.0472 Acc: 0.981 Elapsed: 660.6s ETA: 13.3m\n",
            "[Epoch 4] Batch 2350/5072 Loss: 0.0473 Acc: 0.981 Elapsed: 674.9s ETA: 13.0m\n",
            "[Epoch 4] Batch 2400/5072 Loss: 0.0474 Acc: 0.981 Elapsed: 689.2s ETA: 12.8m\n",
            "[Epoch 4] Batch 2450/5072 Loss: 0.0474 Acc: 0.981 Elapsed: 703.6s ETA: 12.5m\n",
            "[Epoch 4] Batch 2500/5072 Loss: 0.0474 Acc: 0.981 Elapsed: 717.9s ETA: 12.3m\n",
            "[Epoch 4] Batch 2550/5072 Loss: 0.0474 Acc: 0.981 Elapsed: 732.2s ETA: 12.1m\n",
            "[Epoch 4] Batch 2600/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 746.6s ETA: 11.8m\n",
            "[Epoch 4] Batch 2650/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 760.9s ETA: 11.6m\n",
            "[Epoch 4] Batch 2700/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 775.2s ETA: 11.4m\n",
            "[Epoch 4] Batch 2750/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 789.5s ETA: 11.1m\n",
            "[Epoch 4] Batch 2800/5072 Loss: 0.0478 Acc: 0.981 Elapsed: 803.9s ETA: 10.9m\n",
            "[Epoch 4] Batch 2850/5072 Loss: 0.0478 Acc: 0.981 Elapsed: 818.2s ETA: 10.6m\n",
            "[Epoch 4] Batch 2900/5072 Loss: 0.0478 Acc: 0.981 Elapsed: 832.5s ETA: 10.4m\n",
            "[Epoch 4] Batch 2950/5072 Loss: 0.0478 Acc: 0.981 Elapsed: 846.9s ETA: 10.2m\n",
            "[Epoch 4] Batch 3000/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 861.2s ETA: 9.9m\n",
            "[Epoch 4] Batch 3050/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 875.5s ETA: 9.7m\n",
            "[Epoch 4] Batch 3100/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 889.8s ETA: 9.4m\n",
            "[Epoch 4] Batch 3150/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 904.2s ETA: 9.2m\n",
            "[Epoch 4] Batch 3200/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 918.5s ETA: 9.0m\n",
            "[Epoch 4] Batch 3250/5072 Loss: 0.0478 Acc: 0.981 Elapsed: 932.8s ETA: 8.7m\n",
            "[Epoch 4] Batch 3300/5072 Loss: 0.0478 Acc: 0.981 Elapsed: 947.2s ETA: 8.5m\n",
            "[Epoch 4] Batch 3350/5072 Loss: 0.0478 Acc: 0.981 Elapsed: 961.5s ETA: 8.2m\n",
            "[Epoch 4] Batch 3400/5072 Loss: 0.0479 Acc: 0.981 Elapsed: 975.8s ETA: 8.0m\n",
            "[Epoch 4] Batch 3450/5072 Loss: 0.0478 Acc: 0.981 Elapsed: 990.2s ETA: 7.8m\n",
            "[Epoch 4] Batch 3500/5072 Loss: 0.0479 Acc: 0.981 Elapsed: 1004.5s ETA: 7.5m\n",
            "[Epoch 4] Batch 3550/5072 Loss: 0.0479 Acc: 0.981 Elapsed: 1018.8s ETA: 7.3m\n",
            "[Epoch 4] Batch 3600/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 1033.1s ETA: 7.0m\n",
            "[Epoch 4] Batch 3650/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 1047.4s ETA: 6.8m\n",
            "[Epoch 4] Batch 3700/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 1061.7s ETA: 6.6m\n",
            "[Epoch 4] Batch 3750/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1076.0s ETA: 6.3m\n",
            "[Epoch 4] Batch 3800/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1090.3s ETA: 6.1m\n",
            "[Epoch 4] Batch 3850/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1104.6s ETA: 5.8m\n",
            "[Epoch 4] Batch 3900/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1118.9s ETA: 5.6m\n",
            "[Epoch 4] Batch 3950/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 1133.2s ETA: 5.4m\n",
            "[Epoch 4] Batch 4000/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1147.5s ETA: 5.1m\n",
            "[Epoch 4] Batch 4050/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1161.8s ETA: 4.9m\n",
            "[Epoch 4] Batch 4100/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1176.2s ETA: 4.6m\n",
            "[Epoch 4] Batch 4150/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1190.5s ETA: 4.4m\n",
            "[Epoch 4] Batch 4200/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 1204.7s ETA: 4.2m\n",
            "[Epoch 4] Batch 4250/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 1219.1s ETA: 3.9m\n",
            "[Epoch 4] Batch 4300/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 1233.4s ETA: 3.7m\n",
            "[Epoch 4] Batch 4350/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 1247.7s ETA: 3.5m\n",
            "[Epoch 4] Batch 4400/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 1262.1s ETA: 3.2m\n",
            "[Epoch 4] Batch 4450/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 1276.4s ETA: 3.0m\n",
            "[Epoch 4] Batch 4500/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1290.7s ETA: 2.7m\n",
            "[Epoch 4] Batch 4550/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1305.0s ETA: 2.5m\n",
            "[Epoch 4] Batch 4600/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1319.3s ETA: 2.3m\n",
            "[Epoch 4] Batch 4650/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1334.9s ETA: 2.0m\n",
            "[Epoch 4] Batch 4700/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1349.2s ETA: 1.8m\n",
            "[Epoch 4] Batch 4750/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1363.5s ETA: 1.5m\n",
            "[Epoch 4] Batch 4800/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1377.8s ETA: 1.3m\n",
            "[Epoch 4] Batch 4850/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1392.1s ETA: 1.1m\n",
            "[Epoch 4] Batch 4900/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 1406.4s ETA: 0.8m\n",
            "[Epoch 4] Batch 4950/5072 Loss: 0.0477 Acc: 0.981 Elapsed: 1420.7s ETA: 0.6m\n",
            "[Epoch 4] Batch 5000/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1435.0s ETA: 0.3m\n",
            "[Epoch 4] Batch 5050/5072 Loss: 0.0476 Acc: 0.981 Elapsed: 1449.4s ETA: 0.1m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  40%|████      | 4/10 [1:51:56<2:43:36, 1636.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 | Train: loss=0.0476, acc=0.981 | Val:   loss=0.0534, acc=0.978 | time=1579.3s (avg=1670.4s, ETA~167.0m) | ckpt→checkpoints/epoch_4.pth\n",
            "[Epoch 5] Batch 50/5072 Loss: 0.0505 Acc: 0.980 Elapsed: 16.5s ETA: 27.7m\n",
            "[Epoch 5] Batch 100/5072 Loss: 0.0443 Acc: 0.983 Elapsed: 30.8s ETA: 25.5m\n",
            "[Epoch 5] Batch 150/5072 Loss: 0.0462 Acc: 0.982 Elapsed: 45.0s ETA: 24.6m\n",
            "[Epoch 5] Batch 200/5072 Loss: 0.0457 Acc: 0.982 Elapsed: 59.3s ETA: 24.1m\n",
            "[Epoch 5] Batch 250/5072 Loss: 0.0463 Acc: 0.982 Elapsed: 73.6s ETA: 23.7m\n",
            "[Epoch 5] Batch 300/5072 Loss: 0.0468 Acc: 0.982 Elapsed: 87.9s ETA: 23.3m\n",
            "[Epoch 5] Batch 350/5072 Loss: 0.0470 Acc: 0.982 Elapsed: 102.2s ETA: 23.0m\n",
            "[Epoch 5] Batch 400/5072 Loss: 0.0465 Acc: 0.982 Elapsed: 116.5s ETA: 22.7m\n",
            "[Epoch 5] Batch 450/5072 Loss: 0.0461 Acc: 0.982 Elapsed: 130.8s ETA: 22.4m\n",
            "[Epoch 5] Batch 500/5072 Loss: 0.0457 Acc: 0.982 Elapsed: 145.1s ETA: 22.1m\n",
            "[Epoch 5] Batch 550/5072 Loss: 0.0459 Acc: 0.982 Elapsed: 159.3s ETA: 21.8m\n",
            "[Epoch 5] Batch 600/5072 Loss: 0.0458 Acc: 0.982 Elapsed: 173.5s ETA: 21.6m\n",
            "[Epoch 5] Batch 650/5072 Loss: 0.0460 Acc: 0.982 Elapsed: 187.7s ETA: 21.3m\n",
            "[Epoch 5] Batch 700/5072 Loss: 0.0461 Acc: 0.981 Elapsed: 201.9s ETA: 21.0m\n",
            "[Epoch 5] Batch 750/5072 Loss: 0.0463 Acc: 0.981 Elapsed: 216.2s ETA: 20.8m\n",
            "[Epoch 5] Batch 800/5072 Loss: 0.0462 Acc: 0.981 Elapsed: 230.4s ETA: 20.5m\n",
            "[Epoch 5] Batch 850/5072 Loss: 0.0462 Acc: 0.981 Elapsed: 244.6s ETA: 20.3m\n",
            "[Epoch 5] Batch 900/5072 Loss: 0.0463 Acc: 0.981 Elapsed: 258.9s ETA: 20.0m\n",
            "[Epoch 5] Batch 950/5072 Loss: 0.0461 Acc: 0.981 Elapsed: 273.1s ETA: 19.7m\n",
            "[Epoch 5] Batch 1000/5072 Loss: 0.0459 Acc: 0.982 Elapsed: 287.3s ETA: 19.5m\n",
            "[Epoch 5] Batch 1050/5072 Loss: 0.0456 Acc: 0.982 Elapsed: 301.5s ETA: 19.2m\n",
            "[Epoch 5] Batch 1100/5072 Loss: 0.0456 Acc: 0.982 Elapsed: 315.7s ETA: 19.0m\n",
            "[Epoch 5] Batch 1150/5072 Loss: 0.0455 Acc: 0.982 Elapsed: 330.0s ETA: 18.8m\n",
            "[Epoch 5] Batch 1200/5072 Loss: 0.0453 Acc: 0.982 Elapsed: 344.2s ETA: 18.5m\n",
            "[Epoch 5] Batch 1250/5072 Loss: 0.0453 Acc: 0.982 Elapsed: 358.4s ETA: 18.3m\n",
            "[Epoch 5] Batch 1300/5072 Loss: 0.0454 Acc: 0.982 Elapsed: 372.7s ETA: 18.0m\n",
            "[Epoch 5] Batch 1350/5072 Loss: 0.0453 Acc: 0.982 Elapsed: 386.9s ETA: 17.8m\n",
            "[Epoch 5] Batch 1400/5072 Loss: 0.0453 Acc: 0.982 Elapsed: 401.1s ETA: 17.5m\n",
            "[Epoch 5] Batch 1450/5072 Loss: 0.0454 Acc: 0.982 Elapsed: 415.3s ETA: 17.3m\n",
            "[Epoch 5] Batch 1500/5072 Loss: 0.0453 Acc: 0.982 Elapsed: 429.5s ETA: 17.0m\n",
            "[Epoch 5] Batch 1550/5072 Loss: 0.0453 Acc: 0.982 Elapsed: 443.8s ETA: 16.8m\n",
            "[Epoch 5] Batch 1600/5072 Loss: 0.0452 Acc: 0.982 Elapsed: 458.0s ETA: 16.6m\n",
            "[Epoch 5] Batch 1650/5072 Loss: 0.0452 Acc: 0.982 Elapsed: 472.2s ETA: 16.3m\n",
            "[Epoch 5] Batch 1700/5072 Loss: 0.0451 Acc: 0.982 Elapsed: 486.4s ETA: 16.1m\n",
            "[Epoch 5] Batch 1750/5072 Loss: 0.0451 Acc: 0.982 Elapsed: 500.6s ETA: 15.8m\n",
            "[Epoch 5] Batch 1800/5072 Loss: 0.0451 Acc: 0.982 Elapsed: 514.9s ETA: 15.6m\n",
            "[Epoch 5] Batch 1850/5072 Loss: 0.0450 Acc: 0.982 Elapsed: 529.1s ETA: 15.4m\n",
            "[Epoch 5] Batch 1900/5072 Loss: 0.0450 Acc: 0.982 Elapsed: 543.3s ETA: 15.1m\n",
            "[Epoch 5] Batch 1950/5072 Loss: 0.0449 Acc: 0.982 Elapsed: 557.6s ETA: 14.9m\n",
            "[Epoch 5] Batch 2000/5072 Loss: 0.0449 Acc: 0.982 Elapsed: 571.8s ETA: 14.6m\n",
            "[Epoch 5] Batch 2050/5072 Loss: 0.0450 Acc: 0.982 Elapsed: 586.0s ETA: 14.4m\n",
            "[Epoch 5] Batch 2100/5072 Loss: 0.0450 Acc: 0.982 Elapsed: 600.3s ETA: 14.2m\n",
            "[Epoch 5] Batch 2150/5072 Loss: 0.0450 Acc: 0.982 Elapsed: 614.5s ETA: 13.9m\n",
            "[Epoch 5] Batch 2200/5072 Loss: 0.0449 Acc: 0.982 Elapsed: 628.7s ETA: 13.7m\n",
            "[Epoch 5] Batch 2250/5072 Loss: 0.0448 Acc: 0.982 Elapsed: 642.9s ETA: 13.4m\n",
            "[Epoch 5] Batch 2300/5072 Loss: 0.0447 Acc: 0.982 Elapsed: 657.1s ETA: 13.2m\n",
            "[Epoch 5] Batch 2350/5072 Loss: 0.0446 Acc: 0.982 Elapsed: 671.3s ETA: 13.0m\n",
            "[Epoch 5] Batch 2400/5072 Loss: 0.0447 Acc: 0.982 Elapsed: 685.5s ETA: 12.7m\n",
            "[Epoch 5] Batch 2450/5072 Loss: 0.0447 Acc: 0.982 Elapsed: 699.7s ETA: 12.5m\n",
            "[Epoch 5] Batch 2500/5072 Loss: 0.0446 Acc: 0.982 Elapsed: 714.0s ETA: 12.2m\n",
            "[Epoch 5] Batch 2550/5072 Loss: 0.0446 Acc: 0.982 Elapsed: 728.2s ETA: 12.0m\n",
            "[Epoch 5] Batch 2600/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 742.4s ETA: 11.8m\n",
            "[Epoch 5] Batch 2650/5072 Loss: 0.0446 Acc: 0.982 Elapsed: 756.6s ETA: 11.5m\n",
            "[Epoch 5] Batch 2700/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 770.8s ETA: 11.3m\n",
            "[Epoch 5] Batch 2750/5072 Loss: 0.0444 Acc: 0.982 Elapsed: 785.0s ETA: 11.0m\n",
            "[Epoch 5] Batch 2800/5072 Loss: 0.0444 Acc: 0.982 Elapsed: 799.2s ETA: 10.8m\n",
            "[Epoch 5] Batch 2850/5072 Loss: 0.0444 Acc: 0.982 Elapsed: 813.4s ETA: 10.6m\n",
            "[Epoch 5] Batch 2900/5072 Loss: 0.0444 Acc: 0.982 Elapsed: 827.6s ETA: 10.3m\n",
            "[Epoch 5] Batch 2950/5072 Loss: 0.0443 Acc: 0.982 Elapsed: 841.8s ETA: 10.1m\n",
            "[Epoch 5] Batch 3000/5072 Loss: 0.0444 Acc: 0.982 Elapsed: 856.0s ETA: 9.9m\n",
            "[Epoch 5] Batch 3050/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 870.3s ETA: 9.6m\n",
            "[Epoch 5] Batch 3100/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 884.5s ETA: 9.4m\n",
            "[Epoch 5] Batch 3150/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 898.7s ETA: 9.1m\n",
            "[Epoch 5] Batch 3200/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 912.9s ETA: 8.9m\n",
            "[Epoch 5] Batch 3250/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 927.1s ETA: 8.7m\n",
            "[Epoch 5] Batch 3300/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 941.3s ETA: 8.4m\n",
            "[Epoch 5] Batch 3350/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 955.6s ETA: 8.2m\n",
            "[Epoch 5] Batch 3400/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 969.8s ETA: 7.9m\n",
            "[Epoch 5] Batch 3450/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 984.0s ETA: 7.7m\n",
            "[Epoch 5] Batch 3500/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 998.3s ETA: 7.5m\n",
            "[Epoch 5] Batch 3550/5072 Loss: 0.0446 Acc: 0.982 Elapsed: 1012.5s ETA: 7.2m\n",
            "[Epoch 5] Batch 3600/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 1026.7s ETA: 7.0m\n",
            "[Epoch 5] Batch 3650/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 1040.9s ETA: 6.8m\n",
            "[Epoch 5] Batch 3700/5072 Loss: 0.0444 Acc: 0.982 Elapsed: 1055.2s ETA: 6.5m\n",
            "[Epoch 5] Batch 3750/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 1069.4s ETA: 6.3m\n",
            "[Epoch 5] Batch 3800/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 1083.6s ETA: 6.0m\n",
            "[Epoch 5] Batch 3850/5072 Loss: 0.0444 Acc: 0.982 Elapsed: 1097.9s ETA: 5.8m\n",
            "[Epoch 5] Batch 3900/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 1112.1s ETA: 5.6m\n",
            "[Epoch 5] Batch 3950/5072 Loss: 0.0444 Acc: 0.982 Elapsed: 1126.3s ETA: 5.3m\n",
            "[Epoch 5] Batch 4000/5072 Loss: 0.0444 Acc: 0.982 Elapsed: 1140.5s ETA: 5.1m\n",
            "[Epoch 5] Batch 4050/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 1154.8s ETA: 4.9m\n",
            "[Epoch 5] Batch 4100/5072 Loss: 0.0444 Acc: 0.982 Elapsed: 1169.0s ETA: 4.6m\n",
            "[Epoch 5] Batch 4150/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 1183.2s ETA: 4.4m\n",
            "[Epoch 5] Batch 4200/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 1197.4s ETA: 4.1m\n",
            "[Epoch 5] Batch 4250/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 1211.7s ETA: 3.9m\n",
            "[Epoch 5] Batch 4300/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 1225.9s ETA: 3.7m\n",
            "[Epoch 5] Batch 4350/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 1240.1s ETA: 3.4m\n",
            "[Epoch 5] Batch 4400/5072 Loss: 0.0444 Acc: 0.982 Elapsed: 1254.3s ETA: 3.2m\n",
            "[Epoch 5] Batch 4450/5072 Loss: 0.0444 Acc: 0.982 Elapsed: 1268.5s ETA: 3.0m\n",
            "[Epoch 5] Batch 4500/5072 Loss: 0.0443 Acc: 0.982 Elapsed: 1282.7s ETA: 2.7m\n",
            "[Epoch 5] Batch 4550/5072 Loss: 0.0444 Acc: 0.982 Elapsed: 1297.0s ETA: 2.5m\n",
            "[Epoch 5] Batch 4600/5072 Loss: 0.0443 Acc: 0.982 Elapsed: 1311.2s ETA: 2.2m\n",
            "[Epoch 5] Batch 4650/5072 Loss: 0.0443 Acc: 0.982 Elapsed: 1325.4s ETA: 2.0m\n",
            "[Epoch 5] Batch 4700/5072 Loss: 0.0443 Acc: 0.982 Elapsed: 1339.6s ETA: 1.8m\n",
            "[Epoch 5] Batch 4750/5072 Loss: 0.0443 Acc: 0.982 Elapsed: 1353.9s ETA: 1.5m\n",
            "[Epoch 5] Batch 4800/5072 Loss: 0.0443 Acc: 0.982 Elapsed: 1368.1s ETA: 1.3m\n",
            "[Epoch 5] Batch 4850/5072 Loss: 0.0444 Acc: 0.982 Elapsed: 1382.3s ETA: 1.1m\n",
            "[Epoch 5] Batch 4900/5072 Loss: 0.0444 Acc: 0.982 Elapsed: 1396.5s ETA: 0.8m\n",
            "[Epoch 5] Batch 4950/5072 Loss: 0.0444 Acc: 0.982 Elapsed: 1410.7s ETA: 0.6m\n",
            "[Epoch 5] Batch 5000/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 1424.9s ETA: 0.3m\n",
            "[Epoch 5] Batch 5050/5072 Loss: 0.0445 Acc: 0.982 Elapsed: 1439.2s ETA: 0.1m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  50%|█████     | 5/10 [2:18:13<2:14:34, 1614.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 | Train: loss=0.0446, acc=0.982 | Val:   loss=0.0528, acc=0.979 | time=1567.5s (avg=1649.8s, ETA~137.5m) | ckpt→checkpoints/epoch_5.pth\n",
            "[Epoch 6] Batch 50/5072 Loss: 0.0425 Acc: 0.984 Elapsed: 16.6s ETA: 27.7m\n",
            "[Epoch 6] Batch 100/5072 Loss: 0.0400 Acc: 0.985 Elapsed: 30.7s ETA: 25.5m\n",
            "[Epoch 6] Batch 150/5072 Loss: 0.0396 Acc: 0.984 Elapsed: 44.9s ETA: 24.5m\n",
            "[Epoch 6] Batch 200/5072 Loss: 0.0404 Acc: 0.984 Elapsed: 59.1s ETA: 24.0m\n",
            "[Epoch 6] Batch 250/5072 Loss: 0.0418 Acc: 0.984 Elapsed: 73.2s ETA: 23.5m\n",
            "[Epoch 6] Batch 300/5072 Loss: 0.0407 Acc: 0.984 Elapsed: 87.4s ETA: 23.2m\n",
            "[Epoch 6] Batch 350/5072 Loss: 0.0398 Acc: 0.984 Elapsed: 101.6s ETA: 22.8m\n",
            "[Epoch 6] Batch 400/5072 Loss: 0.0406 Acc: 0.984 Elapsed: 115.8s ETA: 22.5m\n",
            "[Epoch 6] Batch 450/5072 Loss: 0.0410 Acc: 0.984 Elapsed: 130.0s ETA: 22.3m\n",
            "[Epoch 6] Batch 500/5072 Loss: 0.0408 Acc: 0.984 Elapsed: 144.2s ETA: 22.0m\n",
            "[Epoch 6] Batch 550/5072 Loss: 0.0412 Acc: 0.983 Elapsed: 158.4s ETA: 21.7m\n",
            "[Epoch 6] Batch 600/5072 Loss: 0.0407 Acc: 0.984 Elapsed: 172.6s ETA: 21.4m\n",
            "[Epoch 6] Batch 650/5072 Loss: 0.0410 Acc: 0.984 Elapsed: 186.7s ETA: 21.2m\n",
            "[Epoch 6] Batch 700/5072 Loss: 0.0408 Acc: 0.984 Elapsed: 200.9s ETA: 20.9m\n",
            "[Epoch 6] Batch 750/5072 Loss: 0.0407 Acc: 0.984 Elapsed: 215.1s ETA: 20.7m\n",
            "[Epoch 6] Batch 800/5072 Loss: 0.0409 Acc: 0.984 Elapsed: 229.3s ETA: 20.4m\n",
            "[Epoch 6] Batch 850/5072 Loss: 0.0406 Acc: 0.984 Elapsed: 243.5s ETA: 20.2m\n",
            "[Epoch 6] Batch 900/5072 Loss: 0.0407 Acc: 0.984 Elapsed: 257.7s ETA: 19.9m\n",
            "[Epoch 6] Batch 950/5072 Loss: 0.0405 Acc: 0.984 Elapsed: 271.9s ETA: 19.7m\n",
            "[Epoch 6] Batch 1000/5072 Loss: 0.0408 Acc: 0.984 Elapsed: 286.1s ETA: 19.4m\n",
            "[Epoch 6] Batch 1050/5072 Loss: 0.0409 Acc: 0.983 Elapsed: 300.3s ETA: 19.2m\n",
            "[Epoch 6] Batch 1100/5072 Loss: 0.0405 Acc: 0.984 Elapsed: 314.5s ETA: 18.9m\n",
            "[Epoch 6] Batch 1150/5072 Loss: 0.0408 Acc: 0.983 Elapsed: 328.7s ETA: 18.7m\n",
            "[Epoch 6] Batch 1200/5072 Loss: 0.0407 Acc: 0.983 Elapsed: 342.9s ETA: 18.4m\n",
            "[Epoch 6] Batch 1250/5072 Loss: 0.0406 Acc: 0.984 Elapsed: 357.0s ETA: 18.2m\n",
            "[Epoch 6] Batch 1300/5072 Loss: 0.0405 Acc: 0.984 Elapsed: 371.2s ETA: 18.0m\n",
            "[Epoch 6] Batch 1350/5072 Loss: 0.0407 Acc: 0.983 Elapsed: 385.5s ETA: 17.7m\n",
            "[Epoch 6] Batch 1400/5072 Loss: 0.0407 Acc: 0.983 Elapsed: 399.7s ETA: 17.5m\n",
            "[Epoch 6] Batch 1450/5072 Loss: 0.0408 Acc: 0.983 Elapsed: 413.9s ETA: 17.2m\n",
            "[Epoch 6] Batch 1500/5072 Loss: 0.0410 Acc: 0.983 Elapsed: 428.1s ETA: 17.0m\n",
            "[Epoch 6] Batch 1550/5072 Loss: 0.0412 Acc: 0.983 Elapsed: 442.3s ETA: 16.7m\n",
            "[Epoch 6] Batch 1600/5072 Loss: 0.0412 Acc: 0.983 Elapsed: 456.5s ETA: 16.5m\n",
            "[Epoch 6] Batch 1650/5072 Loss: 0.0412 Acc: 0.983 Elapsed: 470.7s ETA: 16.3m\n",
            "[Epoch 6] Batch 1700/5072 Loss: 0.0411 Acc: 0.983 Elapsed: 484.9s ETA: 16.0m\n",
            "[Epoch 6] Batch 1750/5072 Loss: 0.0411 Acc: 0.983 Elapsed: 499.1s ETA: 15.8m\n",
            "[Epoch 6] Batch 1800/5072 Loss: 0.0412 Acc: 0.983 Elapsed: 513.3s ETA: 15.6m\n",
            "[Epoch 6] Batch 1850/5072 Loss: 0.0412 Acc: 0.983 Elapsed: 527.5s ETA: 15.3m\n",
            "[Epoch 6] Batch 1900/5072 Loss: 0.0412 Acc: 0.983 Elapsed: 541.7s ETA: 15.1m\n",
            "[Epoch 6] Batch 1950/5072 Loss: 0.0412 Acc: 0.983 Elapsed: 555.9s ETA: 14.8m\n",
            "[Epoch 6] Batch 2000/5072 Loss: 0.0413 Acc: 0.983 Elapsed: 570.1s ETA: 14.6m\n",
            "[Epoch 6] Batch 2050/5072 Loss: 0.0414 Acc: 0.983 Elapsed: 584.3s ETA: 14.4m\n",
            "[Epoch 6] Batch 2100/5072 Loss: 0.0414 Acc: 0.983 Elapsed: 598.5s ETA: 14.1m\n",
            "[Epoch 6] Batch 2150/5072 Loss: 0.0415 Acc: 0.983 Elapsed: 613.9s ETA: 13.9m\n",
            "[Epoch 6] Batch 2200/5072 Loss: 0.0417 Acc: 0.983 Elapsed: 628.1s ETA: 13.7m\n",
            "[Epoch 6] Batch 2250/5072 Loss: 0.0417 Acc: 0.983 Elapsed: 642.3s ETA: 13.4m\n",
            "[Epoch 6] Batch 2300/5072 Loss: 0.0416 Acc: 0.983 Elapsed: 656.5s ETA: 13.2m\n",
            "[Epoch 6] Batch 2350/5072 Loss: 0.0416 Acc: 0.983 Elapsed: 670.7s ETA: 12.9m\n",
            "[Epoch 6] Batch 2400/5072 Loss: 0.0416 Acc: 0.983 Elapsed: 685.0s ETA: 12.7m\n",
            "[Epoch 6] Batch 2450/5072 Loss: 0.0416 Acc: 0.983 Elapsed: 699.2s ETA: 12.5m\n",
            "[Epoch 6] Batch 2500/5072 Loss: 0.0417 Acc: 0.983 Elapsed: 713.4s ETA: 12.2m\n",
            "[Epoch 6] Batch 2550/5072 Loss: 0.0416 Acc: 0.983 Elapsed: 727.6s ETA: 12.0m\n",
            "[Epoch 6] Batch 2600/5072 Loss: 0.0416 Acc: 0.983 Elapsed: 741.8s ETA: 11.8m\n",
            "[Epoch 6] Batch 2650/5072 Loss: 0.0417 Acc: 0.983 Elapsed: 756.0s ETA: 11.5m\n",
            "[Epoch 6] Batch 2700/5072 Loss: 0.0417 Acc: 0.983 Elapsed: 770.3s ETA: 11.3m\n",
            "[Epoch 6] Batch 2750/5072 Loss: 0.0418 Acc: 0.983 Elapsed: 784.5s ETA: 11.0m\n",
            "[Epoch 6] Batch 2800/5072 Loss: 0.0418 Acc: 0.983 Elapsed: 798.7s ETA: 10.8m\n",
            "[Epoch 6] Batch 2850/5072 Loss: 0.0418 Acc: 0.983 Elapsed: 813.0s ETA: 10.6m\n",
            "[Epoch 6] Batch 2900/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 827.2s ETA: 10.3m\n",
            "[Epoch 6] Batch 2950/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 841.4s ETA: 10.1m\n",
            "[Epoch 6] Batch 3000/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 855.6s ETA: 9.8m\n",
            "[Epoch 6] Batch 3050/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 869.8s ETA: 9.6m\n",
            "[Epoch 6] Batch 3100/5072 Loss: 0.0418 Acc: 0.983 Elapsed: 884.0s ETA: 9.4m\n",
            "[Epoch 6] Batch 3150/5072 Loss: 0.0418 Acc: 0.983 Elapsed: 898.2s ETA: 9.1m\n",
            "[Epoch 6] Batch 3200/5072 Loss: 0.0418 Acc: 0.983 Elapsed: 912.4s ETA: 8.9m\n",
            "[Epoch 6] Batch 3250/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 926.6s ETA: 8.7m\n",
            "[Epoch 6] Batch 3300/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 940.8s ETA: 8.4m\n",
            "[Epoch 6] Batch 3350/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 955.0s ETA: 8.2m\n",
            "[Epoch 6] Batch 3400/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 969.3s ETA: 7.9m\n",
            "[Epoch 6] Batch 3450/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 983.5s ETA: 7.7m\n",
            "[Epoch 6] Batch 3500/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 997.7s ETA: 7.5m\n",
            "[Epoch 6] Batch 3550/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1011.9s ETA: 7.2m\n",
            "[Epoch 6] Batch 3600/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1026.1s ETA: 7.0m\n",
            "[Epoch 6] Batch 3650/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1040.3s ETA: 6.8m\n",
            "[Epoch 6] Batch 3700/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1054.5s ETA: 6.5m\n",
            "[Epoch 6] Batch 3750/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1068.7s ETA: 6.3m\n",
            "[Epoch 6] Batch 3800/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1082.9s ETA: 6.0m\n",
            "[Epoch 6] Batch 3850/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1097.1s ETA: 5.8m\n",
            "[Epoch 6] Batch 3900/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1111.3s ETA: 5.6m\n",
            "[Epoch 6] Batch 3950/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1125.5s ETA: 5.3m\n",
            "[Epoch 6] Batch 4000/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1139.8s ETA: 5.1m\n",
            "[Epoch 6] Batch 4050/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1154.0s ETA: 4.9m\n",
            "[Epoch 6] Batch 4100/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1168.2s ETA: 4.6m\n",
            "[Epoch 6] Batch 4150/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1182.4s ETA: 4.4m\n",
            "[Epoch 6] Batch 4200/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1196.6s ETA: 4.1m\n",
            "[Epoch 6] Batch 4250/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1210.8s ETA: 3.9m\n",
            "[Epoch 6] Batch 4300/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1225.0s ETA: 3.7m\n",
            "[Epoch 6] Batch 4350/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1239.2s ETA: 3.4m\n",
            "[Epoch 6] Batch 4400/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1253.5s ETA: 3.2m\n",
            "[Epoch 6] Batch 4450/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1267.7s ETA: 3.0m\n",
            "[Epoch 6] Batch 4500/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1281.9s ETA: 2.7m\n",
            "[Epoch 6] Batch 4550/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1296.1s ETA: 2.5m\n",
            "[Epoch 6] Batch 4600/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1310.3s ETA: 2.2m\n",
            "[Epoch 6] Batch 4650/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1324.6s ETA: 2.0m\n",
            "[Epoch 6] Batch 4700/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1338.8s ETA: 1.8m\n",
            "[Epoch 6] Batch 4750/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1353.0s ETA: 1.5m\n",
            "[Epoch 6] Batch 4800/5072 Loss: 0.0420 Acc: 0.983 Elapsed: 1367.2s ETA: 1.3m\n",
            "[Epoch 6] Batch 4850/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1381.4s ETA: 1.1m\n",
            "[Epoch 6] Batch 4900/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1395.7s ETA: 0.8m\n",
            "[Epoch 6] Batch 4950/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1409.9s ETA: 0.6m\n",
            "[Epoch 6] Batch 5000/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1424.1s ETA: 0.3m\n",
            "[Epoch 6] Batch 5050/5072 Loss: 0.0419 Acc: 0.983 Elapsed: 1438.3s ETA: 0.1m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  60%|██████    | 6/10 [2:44:29<1:46:46, 1601.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 | Train: loss=0.0419, acc=0.983 | Val:   loss=0.0503, acc=0.981 | time=1566.4s (avg=1635.9s, ETA~109.1m) | ckpt→checkpoints/epoch_6.pth\n",
            "[Epoch 7] Batch 50/5072 Loss: 0.0398 Acc: 0.984 Elapsed: 16.5s ETA: 27.7m\n",
            "[Epoch 7] Batch 100/5072 Loss: 0.0405 Acc: 0.983 Elapsed: 30.7s ETA: 25.4m\n",
            "[Epoch 7] Batch 150/5072 Loss: 0.0412 Acc: 0.983 Elapsed: 44.9s ETA: 24.5m\n",
            "[Epoch 7] Batch 200/5072 Loss: 0.0397 Acc: 0.984 Elapsed: 59.0s ETA: 24.0m\n",
            "[Epoch 7] Batch 250/5072 Loss: 0.0394 Acc: 0.984 Elapsed: 73.2s ETA: 23.5m\n",
            "[Epoch 7] Batch 300/5072 Loss: 0.0389 Acc: 0.984 Elapsed: 87.4s ETA: 23.2m\n",
            "[Epoch 7] Batch 350/5072 Loss: 0.0380 Acc: 0.984 Elapsed: 101.6s ETA: 22.9m\n",
            "[Epoch 7] Batch 400/5072 Loss: 0.0376 Acc: 0.985 Elapsed: 115.8s ETA: 22.6m\n",
            "[Epoch 7] Batch 450/5072 Loss: 0.0380 Acc: 0.984 Elapsed: 130.0s ETA: 22.3m\n",
            "[Epoch 7] Batch 500/5072 Loss: 0.0383 Acc: 0.984 Elapsed: 144.2s ETA: 22.0m\n",
            "[Epoch 7] Batch 550/5072 Loss: 0.0379 Acc: 0.984 Elapsed: 158.5s ETA: 21.7m\n",
            "[Epoch 7] Batch 600/5072 Loss: 0.0383 Acc: 0.984 Elapsed: 172.7s ETA: 21.4m\n",
            "[Epoch 7] Batch 650/5072 Loss: 0.0385 Acc: 0.984 Elapsed: 186.9s ETA: 21.2m\n",
            "[Epoch 7] Batch 700/5072 Loss: 0.0382 Acc: 0.984 Elapsed: 201.1s ETA: 20.9m\n",
            "[Epoch 7] Batch 750/5072 Loss: 0.0384 Acc: 0.984 Elapsed: 215.3s ETA: 20.7m\n",
            "[Epoch 7] Batch 800/5072 Loss: 0.0381 Acc: 0.984 Elapsed: 229.5s ETA: 20.4m\n",
            "[Epoch 7] Batch 850/5072 Loss: 0.0383 Acc: 0.984 Elapsed: 243.8s ETA: 20.2m\n",
            "[Epoch 7] Batch 900/5072 Loss: 0.0384 Acc: 0.984 Elapsed: 258.0s ETA: 19.9m\n",
            "[Epoch 7] Batch 950/5072 Loss: 0.0383 Acc: 0.984 Elapsed: 272.2s ETA: 19.7m\n",
            "[Epoch 7] Batch 1000/5072 Loss: 0.0384 Acc: 0.984 Elapsed: 286.4s ETA: 19.4m\n",
            "[Epoch 7] Batch 1050/5072 Loss: 0.0384 Acc: 0.984 Elapsed: 300.6s ETA: 19.2m\n",
            "[Epoch 7] Batch 1100/5072 Loss: 0.0386 Acc: 0.984 Elapsed: 314.8s ETA: 18.9m\n",
            "[Epoch 7] Batch 1150/5072 Loss: 0.0385 Acc: 0.984 Elapsed: 329.0s ETA: 18.7m\n",
            "[Epoch 7] Batch 1200/5072 Loss: 0.0385 Acc: 0.984 Elapsed: 343.3s ETA: 18.5m\n",
            "[Epoch 7] Batch 1250/5072 Loss: 0.0385 Acc: 0.984 Elapsed: 357.5s ETA: 18.2m\n",
            "[Epoch 7] Batch 1300/5072 Loss: 0.0383 Acc: 0.984 Elapsed: 371.7s ETA: 18.0m\n",
            "[Epoch 7] Batch 1350/5072 Loss: 0.0388 Acc: 0.984 Elapsed: 385.9s ETA: 17.7m\n",
            "[Epoch 7] Batch 1400/5072 Loss: 0.0391 Acc: 0.984 Elapsed: 400.1s ETA: 17.5m\n",
            "[Epoch 7] Batch 1450/5072 Loss: 0.0392 Acc: 0.984 Elapsed: 414.3s ETA: 17.2m\n",
            "[Epoch 7] Batch 1500/5072 Loss: 0.0390 Acc: 0.984 Elapsed: 428.5s ETA: 17.0m\n",
            "[Epoch 7] Batch 1550/5072 Loss: 0.0392 Acc: 0.984 Elapsed: 442.7s ETA: 16.8m\n",
            "[Epoch 7] Batch 1600/5072 Loss: 0.0394 Acc: 0.984 Elapsed: 456.9s ETA: 16.5m\n",
            "[Epoch 7] Batch 1650/5072 Loss: 0.0394 Acc: 0.984 Elapsed: 471.2s ETA: 16.3m\n",
            "[Epoch 7] Batch 1700/5072 Loss: 0.0394 Acc: 0.984 Elapsed: 485.4s ETA: 16.0m\n",
            "[Epoch 7] Batch 1750/5072 Loss: 0.0394 Acc: 0.984 Elapsed: 499.6s ETA: 15.8m\n",
            "[Epoch 7] Batch 1800/5072 Loss: 0.0395 Acc: 0.984 Elapsed: 513.8s ETA: 15.6m\n",
            "[Epoch 7] Batch 1850/5072 Loss: 0.0395 Acc: 0.984 Elapsed: 528.0s ETA: 15.3m\n",
            "[Epoch 7] Batch 1900/5072 Loss: 0.0395 Acc: 0.984 Elapsed: 542.2s ETA: 15.1m\n",
            "[Epoch 7] Batch 1950/5072 Loss: 0.0395 Acc: 0.984 Elapsed: 556.4s ETA: 14.8m\n",
            "[Epoch 7] Batch 2000/5072 Loss: 0.0394 Acc: 0.984 Elapsed: 570.6s ETA: 14.6m\n",
            "[Epoch 7] Batch 2050/5072 Loss: 0.0396 Acc: 0.984 Elapsed: 584.8s ETA: 14.4m\n",
            "[Epoch 7] Batch 2100/5072 Loss: 0.0395 Acc: 0.984 Elapsed: 599.0s ETA: 14.1m\n",
            "[Epoch 7] Batch 2150/5072 Loss: 0.0394 Acc: 0.984 Elapsed: 613.2s ETA: 13.9m\n",
            "[Epoch 7] Batch 2200/5072 Loss: 0.0395 Acc: 0.984 Elapsed: 627.4s ETA: 13.7m\n",
            "[Epoch 7] Batch 2250/5072 Loss: 0.0395 Acc: 0.984 Elapsed: 641.6s ETA: 13.4m\n",
            "[Epoch 7] Batch 2300/5072 Loss: 0.0394 Acc: 0.984 Elapsed: 655.8s ETA: 13.2m\n",
            "[Epoch 7] Batch 2350/5072 Loss: 0.0395 Acc: 0.984 Elapsed: 670.0s ETA: 12.9m\n",
            "[Epoch 7] Batch 2400/5072 Loss: 0.0394 Acc: 0.984 Elapsed: 684.2s ETA: 12.7m\n",
            "[Epoch 7] Batch 2450/5072 Loss: 0.0392 Acc: 0.984 Elapsed: 698.4s ETA: 12.5m\n",
            "[Epoch 7] Batch 2500/5072 Loss: 0.0392 Acc: 0.984 Elapsed: 712.6s ETA: 12.2m\n",
            "[Epoch 7] Batch 2550/5072 Loss: 0.0392 Acc: 0.984 Elapsed: 726.8s ETA: 12.0m\n",
            "[Epoch 7] Batch 2600/5072 Loss: 0.0392 Acc: 0.984 Elapsed: 741.0s ETA: 11.7m\n",
            "[Epoch 7] Batch 2650/5072 Loss: 0.0392 Acc: 0.984 Elapsed: 755.2s ETA: 11.5m\n",
            "[Epoch 7] Batch 2700/5072 Loss: 0.0392 Acc: 0.984 Elapsed: 769.4s ETA: 11.3m\n",
            "[Epoch 7] Batch 2750/5072 Loss: 0.0392 Acc: 0.984 Elapsed: 783.6s ETA: 11.0m\n",
            "[Epoch 7] Batch 2800/5072 Loss: 0.0391 Acc: 0.984 Elapsed: 797.9s ETA: 10.8m\n",
            "[Epoch 7] Batch 2850/5072 Loss: 0.0392 Acc: 0.984 Elapsed: 812.1s ETA: 10.6m\n",
            "[Epoch 7] Batch 2900/5072 Loss: 0.0392 Acc: 0.984 Elapsed: 826.3s ETA: 10.3m\n",
            "[Epoch 7] Batch 2950/5072 Loss: 0.0391 Acc: 0.984 Elapsed: 840.4s ETA: 10.1m\n",
            "[Epoch 7] Batch 3000/5072 Loss: 0.0391 Acc: 0.984 Elapsed: 854.6s ETA: 9.8m\n",
            "[Epoch 7] Batch 3050/5072 Loss: 0.0391 Acc: 0.984 Elapsed: 868.8s ETA: 9.6m\n",
            "[Epoch 7] Batch 3100/5072 Loss: 0.0391 Acc: 0.984 Elapsed: 883.0s ETA: 9.4m\n",
            "[Epoch 7] Batch 3150/5072 Loss: 0.0390 Acc: 0.984 Elapsed: 897.2s ETA: 9.1m\n",
            "[Epoch 7] Batch 3200/5072 Loss: 0.0390 Acc: 0.984 Elapsed: 911.4s ETA: 8.9m\n",
            "[Epoch 7] Batch 3250/5072 Loss: 0.0390 Acc: 0.984 Elapsed: 925.6s ETA: 8.6m\n",
            "[Epoch 7] Batch 3300/5072 Loss: 0.0390 Acc: 0.984 Elapsed: 939.8s ETA: 8.4m\n",
            "[Epoch 7] Batch 3350/5072 Loss: 0.0390 Acc: 0.984 Elapsed: 954.0s ETA: 8.2m\n",
            "[Epoch 7] Batch 3400/5072 Loss: 0.0391 Acc: 0.984 Elapsed: 968.3s ETA: 7.9m\n",
            "[Epoch 7] Batch 3450/5072 Loss: 0.0391 Acc: 0.984 Elapsed: 982.5s ETA: 7.7m\n",
            "[Epoch 7] Batch 3500/5072 Loss: 0.0391 Acc: 0.984 Elapsed: 996.7s ETA: 7.5m\n",
            "[Epoch 7] Batch 3550/5072 Loss: 0.0391 Acc: 0.984 Elapsed: 1010.9s ETA: 7.2m\n",
            "[Epoch 7] Batch 3600/5072 Loss: 0.0391 Acc: 0.984 Elapsed: 1025.2s ETA: 7.0m\n",
            "[Epoch 7] Batch 3650/5072 Loss: 0.0390 Acc: 0.984 Elapsed: 1039.4s ETA: 6.7m\n",
            "[Epoch 7] Batch 3700/5072 Loss: 0.0391 Acc: 0.984 Elapsed: 1053.6s ETA: 6.5m\n",
            "[Epoch 7] Batch 3750/5072 Loss: 0.0391 Acc: 0.984 Elapsed: 1067.8s ETA: 6.3m\n",
            "[Epoch 7] Batch 3800/5072 Loss: 0.0390 Acc: 0.984 Elapsed: 1082.0s ETA: 6.0m\n",
            "[Epoch 7] Batch 3850/5072 Loss: 0.0390 Acc: 0.984 Elapsed: 1096.2s ETA: 5.8m\n",
            "[Epoch 7] Batch 3900/5072 Loss: 0.0389 Acc: 0.984 Elapsed: 1110.4s ETA: 5.6m\n",
            "[Epoch 7] Batch 3950/5072 Loss: 0.0389 Acc: 0.984 Elapsed: 1124.6s ETA: 5.3m\n",
            "[Epoch 7] Batch 4000/5072 Loss: 0.0389 Acc: 0.984 Elapsed: 1138.8s ETA: 5.1m\n",
            "[Epoch 7] Batch 4050/5072 Loss: 0.0390 Acc: 0.984 Elapsed: 1153.0s ETA: 4.8m\n",
            "[Epoch 7] Batch 4100/5072 Loss: 0.0389 Acc: 0.984 Elapsed: 1167.2s ETA: 4.6m\n",
            "[Epoch 7] Batch 4150/5072 Loss: 0.0389 Acc: 0.984 Elapsed: 1181.4s ETA: 4.4m\n",
            "[Epoch 7] Batch 4200/5072 Loss: 0.0390 Acc: 0.984 Elapsed: 1195.6s ETA: 4.1m\n",
            "[Epoch 7] Batch 4250/5072 Loss: 0.0390 Acc: 0.984 Elapsed: 1209.8s ETA: 3.9m\n",
            "[Epoch 7] Batch 4300/5072 Loss: 0.0390 Acc: 0.984 Elapsed: 1224.0s ETA: 3.7m\n",
            "[Epoch 7] Batch 4350/5072 Loss: 0.0390 Acc: 0.984 Elapsed: 1238.3s ETA: 3.4m\n",
            "[Epoch 7] Batch 4400/5072 Loss: 0.0389 Acc: 0.984 Elapsed: 1252.5s ETA: 3.2m\n",
            "[Epoch 7] Batch 4450/5072 Loss: 0.0388 Acc: 0.984 Elapsed: 1266.7s ETA: 3.0m\n",
            "[Epoch 7] Batch 4500/5072 Loss: 0.0388 Acc: 0.984 Elapsed: 1280.9s ETA: 2.7m\n",
            "[Epoch 7] Batch 4550/5072 Loss: 0.0388 Acc: 0.984 Elapsed: 1295.1s ETA: 2.5m\n",
            "[Epoch 7] Batch 4600/5072 Loss: 0.0388 Acc: 0.984 Elapsed: 1309.3s ETA: 2.2m\n",
            "[Epoch 7] Batch 4650/5072 Loss: 0.0388 Acc: 0.984 Elapsed: 1323.5s ETA: 2.0m\n",
            "[Epoch 7] Batch 4700/5072 Loss: 0.0389 Acc: 0.984 Elapsed: 1339.0s ETA: 1.8m\n",
            "[Epoch 7] Batch 4750/5072 Loss: 0.0389 Acc: 0.984 Elapsed: 1353.2s ETA: 1.5m\n",
            "[Epoch 7] Batch 4800/5072 Loss: 0.0388 Acc: 0.984 Elapsed: 1367.4s ETA: 1.3m\n",
            "[Epoch 7] Batch 4850/5072 Loss: 0.0389 Acc: 0.984 Elapsed: 1381.6s ETA: 1.1m\n",
            "[Epoch 7] Batch 4900/5072 Loss: 0.0389 Acc: 0.984 Elapsed: 1395.8s ETA: 0.8m\n",
            "[Epoch 7] Batch 4950/5072 Loss: 0.0388 Acc: 0.984 Elapsed: 1410.0s ETA: 0.6m\n",
            "[Epoch 7] Batch 5000/5072 Loss: 0.0388 Acc: 0.984 Elapsed: 1424.2s ETA: 0.3m\n",
            "[Epoch 7] Batch 5050/5072 Loss: 0.0388 Acc: 0.984 Elapsed: 1438.5s ETA: 0.1m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  70%|███████   | 7/10 [3:10:46<1:19:40, 1593.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 | Train: loss=0.0388, acc=0.984 | Val:   loss=0.0504, acc=0.980 | time=1566.2s (avg=1626.0s, ETA~81.3m) | ckpt→checkpoints/epoch_7.pth\n",
            "[Epoch 8] Batch 50/5072 Loss: 0.0367 Acc: 0.985 Elapsed: 17.1s ETA: 28.6m\n",
            "[Epoch 8] Batch 100/5072 Loss: 0.0366 Acc: 0.986 Elapsed: 31.2s ETA: 25.9m\n",
            "[Epoch 8] Batch 150/5072 Loss: 0.0363 Acc: 0.986 Elapsed: 45.4s ETA: 24.8m\n",
            "[Epoch 8] Batch 200/5072 Loss: 0.0359 Acc: 0.986 Elapsed: 59.5s ETA: 24.2m\n",
            "[Epoch 8] Batch 250/5072 Loss: 0.0358 Acc: 0.985 Elapsed: 73.7s ETA: 23.7m\n",
            "[Epoch 8] Batch 300/5072 Loss: 0.0353 Acc: 0.985 Elapsed: 87.9s ETA: 23.3m\n",
            "[Epoch 8] Batch 350/5072 Loss: 0.0356 Acc: 0.985 Elapsed: 102.1s ETA: 23.0m\n",
            "[Epoch 8] Batch 400/5072 Loss: 0.0355 Acc: 0.985 Elapsed: 116.3s ETA: 22.6m\n",
            "[Epoch 8] Batch 450/5072 Loss: 0.0359 Acc: 0.985 Elapsed: 130.6s ETA: 22.4m\n",
            "[Epoch 8] Batch 500/5072 Loss: 0.0355 Acc: 0.985 Elapsed: 144.8s ETA: 22.1m\n",
            "[Epoch 8] Batch 550/5072 Loss: 0.0359 Acc: 0.985 Elapsed: 159.0s ETA: 21.8m\n",
            "[Epoch 8] Batch 600/5072 Loss: 0.0361 Acc: 0.985 Elapsed: 173.2s ETA: 21.5m\n",
            "[Epoch 8] Batch 650/5072 Loss: 0.0356 Acc: 0.985 Elapsed: 187.4s ETA: 21.2m\n",
            "[Epoch 8] Batch 700/5072 Loss: 0.0358 Acc: 0.985 Elapsed: 201.6s ETA: 21.0m\n",
            "[Epoch 8] Batch 750/5072 Loss: 0.0353 Acc: 0.985 Elapsed: 215.8s ETA: 20.7m\n",
            "[Epoch 8] Batch 800/5072 Loss: 0.0353 Acc: 0.985 Elapsed: 230.0s ETA: 20.5m\n",
            "[Epoch 8] Batch 850/5072 Loss: 0.0352 Acc: 0.986 Elapsed: 244.2s ETA: 20.2m\n",
            "[Epoch 8] Batch 900/5072 Loss: 0.0351 Acc: 0.986 Elapsed: 258.4s ETA: 20.0m\n",
            "[Epoch 8] Batch 950/5072 Loss: 0.0351 Acc: 0.986 Elapsed: 272.6s ETA: 19.7m\n",
            "[Epoch 8] Batch 1000/5072 Loss: 0.0349 Acc: 0.986 Elapsed: 286.8s ETA: 19.5m\n",
            "[Epoch 8] Batch 1050/5072 Loss: 0.0350 Acc: 0.986 Elapsed: 301.0s ETA: 19.2m\n",
            "[Epoch 8] Batch 1100/5072 Loss: 0.0354 Acc: 0.986 Elapsed: 315.2s ETA: 19.0m\n",
            "[Epoch 8] Batch 1150/5072 Loss: 0.0354 Acc: 0.986 Elapsed: 329.4s ETA: 18.7m\n",
            "[Epoch 8] Batch 1200/5072 Loss: 0.0353 Acc: 0.985 Elapsed: 343.6s ETA: 18.5m\n",
            "[Epoch 8] Batch 1250/5072 Loss: 0.0354 Acc: 0.985 Elapsed: 357.9s ETA: 18.2m\n",
            "[Epoch 8] Batch 1300/5072 Loss: 0.0355 Acc: 0.985 Elapsed: 372.1s ETA: 18.0m\n",
            "[Epoch 8] Batch 1350/5072 Loss: 0.0355 Acc: 0.985 Elapsed: 386.3s ETA: 17.8m\n",
            "[Epoch 8] Batch 1400/5072 Loss: 0.0354 Acc: 0.986 Elapsed: 400.5s ETA: 17.5m\n",
            "[Epoch 8] Batch 1450/5072 Loss: 0.0356 Acc: 0.985 Elapsed: 414.7s ETA: 17.3m\n",
            "[Epoch 8] Batch 1500/5072 Loss: 0.0355 Acc: 0.985 Elapsed: 428.9s ETA: 17.0m\n",
            "[Epoch 8] Batch 1550/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 443.2s ETA: 16.8m\n",
            "[Epoch 8] Batch 1600/5072 Loss: 0.0358 Acc: 0.985 Elapsed: 457.4s ETA: 16.5m\n",
            "[Epoch 8] Batch 1650/5072 Loss: 0.0359 Acc: 0.985 Elapsed: 471.6s ETA: 16.3m\n",
            "[Epoch 8] Batch 1700/5072 Loss: 0.0360 Acc: 0.985 Elapsed: 485.8s ETA: 16.1m\n",
            "[Epoch 8] Batch 1750/5072 Loss: 0.0360 Acc: 0.985 Elapsed: 500.0s ETA: 15.8m\n",
            "[Epoch 8] Batch 1800/5072 Loss: 0.0360 Acc: 0.985 Elapsed: 514.2s ETA: 15.6m\n",
            "[Epoch 8] Batch 1850/5072 Loss: 0.0360 Acc: 0.985 Elapsed: 528.4s ETA: 15.3m\n",
            "[Epoch 8] Batch 1900/5072 Loss: 0.0359 Acc: 0.985 Elapsed: 542.6s ETA: 15.1m\n",
            "[Epoch 8] Batch 1950/5072 Loss: 0.0361 Acc: 0.985 Elapsed: 556.8s ETA: 14.9m\n",
            "[Epoch 8] Batch 2000/5072 Loss: 0.0361 Acc: 0.985 Elapsed: 571.0s ETA: 14.6m\n",
            "[Epoch 8] Batch 2050/5072 Loss: 0.0361 Acc: 0.985 Elapsed: 585.2s ETA: 14.4m\n",
            "[Epoch 8] Batch 2100/5072 Loss: 0.0360 Acc: 0.985 Elapsed: 599.4s ETA: 14.1m\n",
            "[Epoch 8] Batch 2150/5072 Loss: 0.0361 Acc: 0.985 Elapsed: 613.6s ETA: 13.9m\n",
            "[Epoch 8] Batch 2200/5072 Loss: 0.0360 Acc: 0.985 Elapsed: 627.8s ETA: 13.7m\n",
            "[Epoch 8] Batch 2250/5072 Loss: 0.0360 Acc: 0.985 Elapsed: 642.0s ETA: 13.4m\n",
            "[Epoch 8] Batch 2300/5072 Loss: 0.0360 Acc: 0.985 Elapsed: 656.2s ETA: 13.2m\n",
            "[Epoch 8] Batch 2350/5072 Loss: 0.0360 Acc: 0.985 Elapsed: 670.3s ETA: 12.9m\n",
            "[Epoch 8] Batch 2400/5072 Loss: 0.0360 Acc: 0.985 Elapsed: 684.5s ETA: 12.7m\n",
            "[Epoch 8] Batch 2450/5072 Loss: 0.0359 Acc: 0.985 Elapsed: 698.7s ETA: 12.5m\n",
            "[Epoch 8] Batch 2500/5072 Loss: 0.0359 Acc: 0.985 Elapsed: 712.9s ETA: 12.2m\n",
            "[Epoch 8] Batch 2550/5072 Loss: 0.0358 Acc: 0.985 Elapsed: 727.1s ETA: 12.0m\n",
            "[Epoch 8] Batch 2600/5072 Loss: 0.0359 Acc: 0.985 Elapsed: 741.3s ETA: 11.7m\n",
            "[Epoch 8] Batch 2650/5072 Loss: 0.0359 Acc: 0.985 Elapsed: 755.5s ETA: 11.5m\n",
            "[Epoch 8] Batch 2700/5072 Loss: 0.0359 Acc: 0.985 Elapsed: 769.7s ETA: 11.3m\n",
            "[Epoch 8] Batch 2750/5072 Loss: 0.0360 Acc: 0.985 Elapsed: 783.8s ETA: 11.0m\n",
            "[Epoch 8] Batch 2800/5072 Loss: 0.0360 Acc: 0.985 Elapsed: 798.0s ETA: 10.8m\n",
            "[Epoch 8] Batch 2850/5072 Loss: 0.0359 Acc: 0.985 Elapsed: 812.2s ETA: 10.6m\n",
            "[Epoch 8] Batch 2900/5072 Loss: 0.0359 Acc: 0.985 Elapsed: 826.4s ETA: 10.3m\n",
            "[Epoch 8] Batch 2950/5072 Loss: 0.0359 Acc: 0.985 Elapsed: 840.6s ETA: 10.1m\n",
            "[Epoch 8] Batch 3000/5072 Loss: 0.0359 Acc: 0.985 Elapsed: 854.8s ETA: 9.8m\n",
            "[Epoch 8] Batch 3050/5072 Loss: 0.0359 Acc: 0.985 Elapsed: 869.0s ETA: 9.6m\n",
            "[Epoch 8] Batch 3100/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 883.2s ETA: 9.4m\n",
            "[Epoch 8] Batch 3150/5072 Loss: 0.0358 Acc: 0.985 Elapsed: 897.4s ETA: 9.1m\n",
            "[Epoch 8] Batch 3200/5072 Loss: 0.0358 Acc: 0.985 Elapsed: 911.6s ETA: 8.9m\n",
            "[Epoch 8] Batch 3250/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 925.8s ETA: 8.6m\n",
            "[Epoch 8] Batch 3300/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 939.9s ETA: 8.4m\n",
            "[Epoch 8] Batch 3350/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 954.1s ETA: 8.2m\n",
            "[Epoch 8] Batch 3400/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 968.3s ETA: 7.9m\n",
            "[Epoch 8] Batch 3450/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 982.5s ETA: 7.7m\n",
            "[Epoch 8] Batch 3500/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 996.7s ETA: 7.5m\n",
            "[Epoch 8] Batch 3550/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1010.9s ETA: 7.2m\n",
            "[Epoch 8] Batch 3600/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1025.1s ETA: 7.0m\n",
            "[Epoch 8] Batch 3650/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1039.3s ETA: 6.7m\n",
            "[Epoch 8] Batch 3700/5072 Loss: 0.0356 Acc: 0.985 Elapsed: 1053.5s ETA: 6.5m\n",
            "[Epoch 8] Batch 3750/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1067.7s ETA: 6.3m\n",
            "[Epoch 8] Batch 3800/5072 Loss: 0.0356 Acc: 0.985 Elapsed: 1081.9s ETA: 6.0m\n",
            "[Epoch 8] Batch 3850/5072 Loss: 0.0356 Acc: 0.985 Elapsed: 1096.1s ETA: 5.8m\n",
            "[Epoch 8] Batch 3900/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1110.3s ETA: 5.6m\n",
            "[Epoch 8] Batch 3950/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1124.5s ETA: 5.3m\n",
            "[Epoch 8] Batch 4000/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1138.7s ETA: 5.1m\n",
            "[Epoch 8] Batch 4050/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1152.9s ETA: 4.8m\n",
            "[Epoch 8] Batch 4100/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1167.2s ETA: 4.6m\n",
            "[Epoch 8] Batch 4150/5072 Loss: 0.0358 Acc: 0.985 Elapsed: 1181.4s ETA: 4.4m\n",
            "[Epoch 8] Batch 4200/5072 Loss: 0.0358 Acc: 0.985 Elapsed: 1195.6s ETA: 4.1m\n",
            "[Epoch 8] Batch 4250/5072 Loss: 0.0358 Acc: 0.985 Elapsed: 1209.8s ETA: 3.9m\n",
            "[Epoch 8] Batch 4300/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1224.0s ETA: 3.7m\n",
            "[Epoch 8] Batch 4350/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1238.2s ETA: 3.4m\n",
            "[Epoch 8] Batch 4400/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1252.4s ETA: 3.2m\n",
            "[Epoch 8] Batch 4450/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1266.6s ETA: 3.0m\n",
            "[Epoch 8] Batch 4500/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1280.8s ETA: 2.7m\n",
            "[Epoch 8] Batch 4550/5072 Loss: 0.0358 Acc: 0.985 Elapsed: 1295.0s ETA: 2.5m\n",
            "[Epoch 8] Batch 4600/5072 Loss: 0.0358 Acc: 0.985 Elapsed: 1309.3s ETA: 2.2m\n",
            "[Epoch 8] Batch 4650/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1323.5s ETA: 2.0m\n",
            "[Epoch 8] Batch 4700/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1337.7s ETA: 1.8m\n",
            "[Epoch 8] Batch 4750/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1351.9s ETA: 1.5m\n",
            "[Epoch 8] Batch 4800/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1366.1s ETA: 1.3m\n",
            "[Epoch 8] Batch 4850/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1380.3s ETA: 1.1m\n",
            "[Epoch 8] Batch 4900/5072 Loss: 0.0356 Acc: 0.985 Elapsed: 1394.5s ETA: 0.8m\n",
            "[Epoch 8] Batch 4950/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1408.7s ETA: 0.6m\n",
            "[Epoch 8] Batch 5000/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1423.0s ETA: 0.3m\n",
            "[Epoch 8] Batch 5050/5072 Loss: 0.0357 Acc: 0.985 Elapsed: 1437.2s ETA: 0.1m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  80%|████████  | 8/10 [3:37:00<52:54, 1587.35s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 | Train: loss=0.0358, acc=0.985 | Val:   loss=0.0507, acc=0.981 | time=1565.2s (avg=1618.4s, ETA~53.9m) | ckpt→checkpoints/epoch_8.pth\n",
            "[Epoch 9] Batch 50/5072 Loss: 0.0376 Acc: 0.984 Elapsed: 16.7s ETA: 28.0m\n",
            "[Epoch 9] Batch 100/5072 Loss: 0.0316 Acc: 0.987 Elapsed: 30.9s ETA: 25.6m\n",
            "[Epoch 9] Batch 150/5072 Loss: 0.0323 Acc: 0.987 Elapsed: 45.1s ETA: 24.7m\n",
            "[Epoch 9] Batch 200/5072 Loss: 0.0320 Acc: 0.987 Elapsed: 59.4s ETA: 24.1m\n",
            "[Epoch 9] Batch 250/5072 Loss: 0.0328 Acc: 0.987 Elapsed: 73.6s ETA: 23.7m\n",
            "[Epoch 9] Batch 300/5072 Loss: 0.0321 Acc: 0.987 Elapsed: 87.8s ETA: 23.3m\n",
            "[Epoch 9] Batch 350/5072 Loss: 0.0332 Acc: 0.987 Elapsed: 102.1s ETA: 23.0m\n",
            "[Epoch 9] Batch 400/5072 Loss: 0.0331 Acc: 0.987 Elapsed: 116.3s ETA: 22.6m\n",
            "[Epoch 9] Batch 450/5072 Loss: 0.0326 Acc: 0.987 Elapsed: 130.6s ETA: 22.4m\n",
            "[Epoch 9] Batch 500/5072 Loss: 0.0325 Acc: 0.987 Elapsed: 144.8s ETA: 22.1m\n",
            "[Epoch 9] Batch 550/5072 Loss: 0.0326 Acc: 0.987 Elapsed: 159.1s ETA: 21.8m\n",
            "[Epoch 9] Batch 600/5072 Loss: 0.0326 Acc: 0.987 Elapsed: 173.4s ETA: 21.5m\n",
            "[Epoch 9] Batch 650/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 187.6s ETA: 21.3m\n",
            "[Epoch 9] Batch 700/5072 Loss: 0.0333 Acc: 0.986 Elapsed: 201.9s ETA: 21.0m\n",
            "[Epoch 9] Batch 750/5072 Loss: 0.0331 Acc: 0.987 Elapsed: 216.2s ETA: 20.8m\n",
            "[Epoch 9] Batch 800/5072 Loss: 0.0331 Acc: 0.987 Elapsed: 230.4s ETA: 20.5m\n",
            "[Epoch 9] Batch 850/5072 Loss: 0.0332 Acc: 0.987 Elapsed: 244.7s ETA: 20.3m\n",
            "[Epoch 9] Batch 900/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 258.9s ETA: 20.0m\n",
            "[Epoch 9] Batch 950/5072 Loss: 0.0333 Acc: 0.986 Elapsed: 273.2s ETA: 19.8m\n",
            "[Epoch 9] Batch 1000/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 287.5s ETA: 19.5m\n",
            "[Epoch 9] Batch 1050/5072 Loss: 0.0335 Acc: 0.986 Elapsed: 301.8s ETA: 19.3m\n",
            "[Epoch 9] Batch 1100/5072 Loss: 0.0334 Acc: 0.986 Elapsed: 316.0s ETA: 19.0m\n",
            "[Epoch 9] Batch 1150/5072 Loss: 0.0333 Acc: 0.986 Elapsed: 330.3s ETA: 18.8m\n",
            "[Epoch 9] Batch 1200/5072 Loss: 0.0333 Acc: 0.986 Elapsed: 344.5s ETA: 18.5m\n",
            "[Epoch 9] Batch 1250/5072 Loss: 0.0334 Acc: 0.986 Elapsed: 358.8s ETA: 18.3m\n",
            "[Epoch 9] Batch 1300/5072 Loss: 0.0330 Acc: 0.987 Elapsed: 373.0s ETA: 18.0m\n",
            "[Epoch 9] Batch 1350/5072 Loss: 0.0328 Acc: 0.987 Elapsed: 387.3s ETA: 17.8m\n",
            "[Epoch 9] Batch 1400/5072 Loss: 0.0327 Acc: 0.987 Elapsed: 401.5s ETA: 17.6m\n",
            "[Epoch 9] Batch 1450/5072 Loss: 0.0326 Acc: 0.987 Elapsed: 415.8s ETA: 17.3m\n",
            "[Epoch 9] Batch 1500/5072 Loss: 0.0325 Acc: 0.987 Elapsed: 430.1s ETA: 17.1m\n",
            "[Epoch 9] Batch 1550/5072 Loss: 0.0326 Acc: 0.987 Elapsed: 444.3s ETA: 16.8m\n",
            "[Epoch 9] Batch 1600/5072 Loss: 0.0327 Acc: 0.987 Elapsed: 458.6s ETA: 16.6m\n",
            "[Epoch 9] Batch 1650/5072 Loss: 0.0327 Acc: 0.987 Elapsed: 472.8s ETA: 16.3m\n",
            "[Epoch 9] Batch 1700/5072 Loss: 0.0328 Acc: 0.986 Elapsed: 487.0s ETA: 16.1m\n",
            "[Epoch 9] Batch 1750/5072 Loss: 0.0328 Acc: 0.986 Elapsed: 501.3s ETA: 15.9m\n",
            "[Epoch 9] Batch 1800/5072 Loss: 0.0327 Acc: 0.986 Elapsed: 515.6s ETA: 15.6m\n",
            "[Epoch 9] Batch 1850/5072 Loss: 0.0327 Acc: 0.986 Elapsed: 529.8s ETA: 15.4m\n",
            "[Epoch 9] Batch 1900/5072 Loss: 0.0327 Acc: 0.987 Elapsed: 544.1s ETA: 15.1m\n",
            "[Epoch 9] Batch 1950/5072 Loss: 0.0326 Acc: 0.987 Elapsed: 558.3s ETA: 14.9m\n",
            "[Epoch 9] Batch 2000/5072 Loss: 0.0326 Acc: 0.987 Elapsed: 572.6s ETA: 14.7m\n",
            "[Epoch 9] Batch 2050/5072 Loss: 0.0327 Acc: 0.987 Elapsed: 586.8s ETA: 14.4m\n",
            "[Epoch 9] Batch 2100/5072 Loss: 0.0328 Acc: 0.986 Elapsed: 601.0s ETA: 14.2m\n",
            "[Epoch 9] Batch 2150/5072 Loss: 0.0327 Acc: 0.986 Elapsed: 615.2s ETA: 13.9m\n",
            "[Epoch 9] Batch 2200/5072 Loss: 0.0326 Acc: 0.987 Elapsed: 630.7s ETA: 13.7m\n",
            "[Epoch 9] Batch 2250/5072 Loss: 0.0326 Acc: 0.987 Elapsed: 644.9s ETA: 13.5m\n",
            "[Epoch 9] Batch 2300/5072 Loss: 0.0325 Acc: 0.987 Elapsed: 659.0s ETA: 13.2m\n",
            "[Epoch 9] Batch 2350/5072 Loss: 0.0324 Acc: 0.987 Elapsed: 673.2s ETA: 13.0m\n",
            "[Epoch 9] Batch 2400/5072 Loss: 0.0325 Acc: 0.987 Elapsed: 687.4s ETA: 12.8m\n",
            "[Epoch 9] Batch 2450/5072 Loss: 0.0326 Acc: 0.987 Elapsed: 701.6s ETA: 12.5m\n",
            "[Epoch 9] Batch 2500/5072 Loss: 0.0326 Acc: 0.987 Elapsed: 715.8s ETA: 12.3m\n",
            "[Epoch 9] Batch 2550/5072 Loss: 0.0327 Acc: 0.987 Elapsed: 730.1s ETA: 12.0m\n",
            "[Epoch 9] Batch 2600/5072 Loss: 0.0328 Acc: 0.986 Elapsed: 744.3s ETA: 11.8m\n",
            "[Epoch 9] Batch 2650/5072 Loss: 0.0328 Acc: 0.986 Elapsed: 758.5s ETA: 11.6m\n",
            "[Epoch 9] Batch 2700/5072 Loss: 0.0328 Acc: 0.986 Elapsed: 772.7s ETA: 11.3m\n",
            "[Epoch 9] Batch 2750/5072 Loss: 0.0329 Acc: 0.986 Elapsed: 786.9s ETA: 11.1m\n",
            "[Epoch 9] Batch 2800/5072 Loss: 0.0328 Acc: 0.986 Elapsed: 801.1s ETA: 10.8m\n",
            "[Epoch 9] Batch 2850/5072 Loss: 0.0330 Acc: 0.986 Elapsed: 815.3s ETA: 10.6m\n",
            "[Epoch 9] Batch 2900/5072 Loss: 0.0330 Acc: 0.986 Elapsed: 829.6s ETA: 10.4m\n",
            "[Epoch 9] Batch 2950/5072 Loss: 0.0330 Acc: 0.986 Elapsed: 843.8s ETA: 10.1m\n",
            "[Epoch 9] Batch 3000/5072 Loss: 0.0331 Acc: 0.986 Elapsed: 858.0s ETA: 9.9m\n",
            "[Epoch 9] Batch 3050/5072 Loss: 0.0331 Acc: 0.986 Elapsed: 872.2s ETA: 9.6m\n",
            "[Epoch 9] Batch 3100/5072 Loss: 0.0330 Acc: 0.986 Elapsed: 886.4s ETA: 9.4m\n",
            "[Epoch 9] Batch 3150/5072 Loss: 0.0331 Acc: 0.986 Elapsed: 900.6s ETA: 9.2m\n",
            "[Epoch 9] Batch 3200/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 914.8s ETA: 8.9m\n",
            "[Epoch 9] Batch 3250/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 929.1s ETA: 8.7m\n",
            "[Epoch 9] Batch 3300/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 943.3s ETA: 8.4m\n",
            "[Epoch 9] Batch 3350/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 957.5s ETA: 8.2m\n",
            "[Epoch 9] Batch 3400/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 971.7s ETA: 8.0m\n",
            "[Epoch 9] Batch 3450/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 985.9s ETA: 7.7m\n",
            "[Epoch 9] Batch 3500/5072 Loss: 0.0333 Acc: 0.986 Elapsed: 1000.1s ETA: 7.5m\n",
            "[Epoch 9] Batch 3550/5072 Loss: 0.0333 Acc: 0.986 Elapsed: 1014.3s ETA: 7.2m\n",
            "[Epoch 9] Batch 3600/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1028.5s ETA: 7.0m\n",
            "[Epoch 9] Batch 3650/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1042.7s ETA: 6.8m\n",
            "[Epoch 9] Batch 3700/5072 Loss: 0.0333 Acc: 0.986 Elapsed: 1057.0s ETA: 6.5m\n",
            "[Epoch 9] Batch 3750/5072 Loss: 0.0333 Acc: 0.986 Elapsed: 1071.1s ETA: 6.3m\n",
            "[Epoch 9] Batch 3800/5072 Loss: 0.0333 Acc: 0.986 Elapsed: 1085.4s ETA: 6.1m\n",
            "[Epoch 9] Batch 3850/5072 Loss: 0.0333 Acc: 0.986 Elapsed: 1099.6s ETA: 5.8m\n",
            "[Epoch 9] Batch 3900/5072 Loss: 0.0333 Acc: 0.986 Elapsed: 1113.8s ETA: 5.6m\n",
            "[Epoch 9] Batch 3950/5072 Loss: 0.0333 Acc: 0.986 Elapsed: 1128.0s ETA: 5.3m\n",
            "[Epoch 9] Batch 4000/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1142.3s ETA: 5.1m\n",
            "[Epoch 9] Batch 4050/5072 Loss: 0.0331 Acc: 0.986 Elapsed: 1156.5s ETA: 4.9m\n",
            "[Epoch 9] Batch 4100/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1170.7s ETA: 4.6m\n",
            "[Epoch 9] Batch 4150/5072 Loss: 0.0331 Acc: 0.986 Elapsed: 1184.9s ETA: 4.4m\n",
            "[Epoch 9] Batch 4200/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1199.0s ETA: 4.1m\n",
            "[Epoch 9] Batch 4250/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1213.2s ETA: 3.9m\n",
            "[Epoch 9] Batch 4300/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1227.4s ETA: 3.7m\n",
            "[Epoch 9] Batch 4350/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1241.6s ETA: 3.4m\n",
            "[Epoch 9] Batch 4400/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1255.7s ETA: 3.2m\n",
            "[Epoch 9] Batch 4450/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1269.9s ETA: 3.0m\n",
            "[Epoch 9] Batch 4500/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1284.1s ETA: 2.7m\n",
            "[Epoch 9] Batch 4550/5072 Loss: 0.0333 Acc: 0.986 Elapsed: 1298.3s ETA: 2.5m\n",
            "[Epoch 9] Batch 4600/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1312.5s ETA: 2.2m\n",
            "[Epoch 9] Batch 4650/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1326.6s ETA: 2.0m\n",
            "[Epoch 9] Batch 4700/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1340.8s ETA: 1.8m\n",
            "[Epoch 9] Batch 4750/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1355.0s ETA: 1.5m\n",
            "[Epoch 9] Batch 4800/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1369.2s ETA: 1.3m\n",
            "[Epoch 9] Batch 4850/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1383.4s ETA: 1.1m\n",
            "[Epoch 9] Batch 4900/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1397.5s ETA: 0.8m\n",
            "[Epoch 9] Batch 4950/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1411.7s ETA: 0.6m\n",
            "[Epoch 9] Batch 5000/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1425.9s ETA: 0.3m\n",
            "[Epoch 9] Batch 5050/5072 Loss: 0.0332 Acc: 0.986 Elapsed: 1440.1s ETA: 0.1m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  90%|█████████ | 9/10 [4:03:19<26:24, 1584.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 | Train: loss=0.0332, acc=0.986 | Val:   loss=0.0522, acc=0.981 | time=1568.1s (avg=1612.8s, ETA~26.9m) | ckpt→checkpoints/epoch_9.pth\n",
            "[Epoch 10] Batch 50/5072 Loss: 0.0334 Acc: 0.987 Elapsed: 16.8s ETA: 28.1m\n",
            "[Epoch 10] Batch 100/5072 Loss: 0.0341 Acc: 0.986 Elapsed: 30.9s ETA: 25.6m\n",
            "[Epoch 10] Batch 150/5072 Loss: 0.0320 Acc: 0.987 Elapsed: 45.1s ETA: 24.6m\n",
            "[Epoch 10] Batch 200/5072 Loss: 0.0318 Acc: 0.987 Elapsed: 59.2s ETA: 24.0m\n",
            "[Epoch 10] Batch 250/5072 Loss: 0.0309 Acc: 0.988 Elapsed: 73.4s ETA: 23.6m\n",
            "[Epoch 10] Batch 300/5072 Loss: 0.0323 Acc: 0.987 Elapsed: 87.6s ETA: 23.2m\n",
            "[Epoch 10] Batch 350/5072 Loss: 0.0321 Acc: 0.987 Elapsed: 101.7s ETA: 22.9m\n",
            "[Epoch 10] Batch 400/5072 Loss: 0.0317 Acc: 0.987 Elapsed: 115.9s ETA: 22.6m\n",
            "[Epoch 10] Batch 450/5072 Loss: 0.0317 Acc: 0.987 Elapsed: 130.1s ETA: 22.3m\n",
            "[Epoch 10] Batch 500/5072 Loss: 0.0317 Acc: 0.987 Elapsed: 144.2s ETA: 22.0m\n",
            "[Epoch 10] Batch 550/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 158.4s ETA: 21.7m\n",
            "[Epoch 10] Batch 600/5072 Loss: 0.0313 Acc: 0.987 Elapsed: 172.6s ETA: 21.4m\n",
            "[Epoch 10] Batch 650/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 186.7s ETA: 21.2m\n",
            "[Epoch 10] Batch 700/5072 Loss: 0.0316 Acc: 0.987 Elapsed: 200.9s ETA: 20.9m\n",
            "[Epoch 10] Batch 750/5072 Loss: 0.0312 Acc: 0.987 Elapsed: 215.1s ETA: 20.7m\n",
            "[Epoch 10] Batch 800/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 229.2s ETA: 20.4m\n",
            "[Epoch 10] Batch 850/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 243.4s ETA: 20.2m\n",
            "[Epoch 10] Batch 900/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 257.6s ETA: 19.9m\n",
            "[Epoch 10] Batch 950/5072 Loss: 0.0316 Acc: 0.987 Elapsed: 271.8s ETA: 19.7m\n",
            "[Epoch 10] Batch 1000/5072 Loss: 0.0317 Acc: 0.987 Elapsed: 285.9s ETA: 19.4m\n",
            "[Epoch 10] Batch 1050/5072 Loss: 0.0316 Acc: 0.987 Elapsed: 300.1s ETA: 19.2m\n",
            "[Epoch 10] Batch 1100/5072 Loss: 0.0317 Acc: 0.987 Elapsed: 314.3s ETA: 18.9m\n",
            "[Epoch 10] Batch 1150/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 328.5s ETA: 18.7m\n",
            "[Epoch 10] Batch 1200/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 342.7s ETA: 18.4m\n",
            "[Epoch 10] Batch 1250/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 356.9s ETA: 18.2m\n",
            "[Epoch 10] Batch 1300/5072 Loss: 0.0313 Acc: 0.987 Elapsed: 371.1s ETA: 17.9m\n",
            "[Epoch 10] Batch 1350/5072 Loss: 0.0311 Acc: 0.987 Elapsed: 385.3s ETA: 17.7m\n",
            "[Epoch 10] Batch 1400/5072 Loss: 0.0313 Acc: 0.987 Elapsed: 399.5s ETA: 17.5m\n",
            "[Epoch 10] Batch 1450/5072 Loss: 0.0313 Acc: 0.987 Elapsed: 413.7s ETA: 17.2m\n",
            "[Epoch 10] Batch 1500/5072 Loss: 0.0312 Acc: 0.987 Elapsed: 427.8s ETA: 17.0m\n",
            "[Epoch 10] Batch 1550/5072 Loss: 0.0312 Acc: 0.987 Elapsed: 442.0s ETA: 16.7m\n",
            "[Epoch 10] Batch 1600/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 456.2s ETA: 16.5m\n",
            "[Epoch 10] Batch 1650/5072 Loss: 0.0312 Acc: 0.987 Elapsed: 470.4s ETA: 16.3m\n",
            "[Epoch 10] Batch 1700/5072 Loss: 0.0312 Acc: 0.987 Elapsed: 484.6s ETA: 16.0m\n",
            "[Epoch 10] Batch 1750/5072 Loss: 0.0311 Acc: 0.987 Elapsed: 498.8s ETA: 15.8m\n",
            "[Epoch 10] Batch 1800/5072 Loss: 0.0312 Acc: 0.987 Elapsed: 513.0s ETA: 15.5m\n",
            "[Epoch 10] Batch 1850/5072 Loss: 0.0310 Acc: 0.987 Elapsed: 527.2s ETA: 15.3m\n",
            "[Epoch 10] Batch 1900/5072 Loss: 0.0311 Acc: 0.987 Elapsed: 541.3s ETA: 15.1m\n",
            "[Epoch 10] Batch 1950/5072 Loss: 0.0310 Acc: 0.987 Elapsed: 555.5s ETA: 14.8m\n",
            "[Epoch 10] Batch 2000/5072 Loss: 0.0309 Acc: 0.987 Elapsed: 569.7s ETA: 14.6m\n",
            "[Epoch 10] Batch 2050/5072 Loss: 0.0309 Acc: 0.987 Elapsed: 583.9s ETA: 14.3m\n",
            "[Epoch 10] Batch 2100/5072 Loss: 0.0309 Acc: 0.987 Elapsed: 598.1s ETA: 14.1m\n",
            "[Epoch 10] Batch 2150/5072 Loss: 0.0308 Acc: 0.987 Elapsed: 612.3s ETA: 13.9m\n",
            "[Epoch 10] Batch 2200/5072 Loss: 0.0307 Acc: 0.987 Elapsed: 626.5s ETA: 13.6m\n",
            "[Epoch 10] Batch 2250/5072 Loss: 0.0307 Acc: 0.987 Elapsed: 640.6s ETA: 13.4m\n",
            "[Epoch 10] Batch 2300/5072 Loss: 0.0307 Acc: 0.987 Elapsed: 654.8s ETA: 13.2m\n",
            "[Epoch 10] Batch 2350/5072 Loss: 0.0306 Acc: 0.987 Elapsed: 669.0s ETA: 12.9m\n",
            "[Epoch 10] Batch 2400/5072 Loss: 0.0307 Acc: 0.987 Elapsed: 683.2s ETA: 12.7m\n",
            "[Epoch 10] Batch 2450/5072 Loss: 0.0308 Acc: 0.987 Elapsed: 697.4s ETA: 12.4m\n",
            "[Epoch 10] Batch 2500/5072 Loss: 0.0308 Acc: 0.987 Elapsed: 711.6s ETA: 12.2m\n",
            "[Epoch 10] Batch 2550/5072 Loss: 0.0308 Acc: 0.987 Elapsed: 725.8s ETA: 12.0m\n",
            "[Epoch 10] Batch 2600/5072 Loss: 0.0309 Acc: 0.987 Elapsed: 740.0s ETA: 11.7m\n",
            "[Epoch 10] Batch 2650/5072 Loss: 0.0309 Acc: 0.987 Elapsed: 754.1s ETA: 11.5m\n",
            "[Epoch 10] Batch 2700/5072 Loss: 0.0309 Acc: 0.987 Elapsed: 768.3s ETA: 11.2m\n",
            "[Epoch 10] Batch 2750/5072 Loss: 0.0310 Acc: 0.987 Elapsed: 782.5s ETA: 11.0m\n",
            "[Epoch 10] Batch 2800/5072 Loss: 0.0310 Acc: 0.987 Elapsed: 796.7s ETA: 10.8m\n",
            "[Epoch 10] Batch 2850/5072 Loss: 0.0311 Acc: 0.987 Elapsed: 810.9s ETA: 10.5m\n",
            "[Epoch 10] Batch 2900/5072 Loss: 0.0311 Acc: 0.987 Elapsed: 825.1s ETA: 10.3m\n",
            "[Epoch 10] Batch 2950/5072 Loss: 0.0312 Acc: 0.987 Elapsed: 839.3s ETA: 10.1m\n",
            "[Epoch 10] Batch 3000/5072 Loss: 0.0312 Acc: 0.987 Elapsed: 853.5s ETA: 9.8m\n",
            "[Epoch 10] Batch 3050/5072 Loss: 0.0311 Acc: 0.987 Elapsed: 867.7s ETA: 9.6m\n",
            "[Epoch 10] Batch 3100/5072 Loss: 0.0312 Acc: 0.987 Elapsed: 881.9s ETA: 9.3m\n",
            "[Epoch 10] Batch 3150/5072 Loss: 0.0313 Acc: 0.987 Elapsed: 896.1s ETA: 9.1m\n",
            "[Epoch 10] Batch 3200/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 910.3s ETA: 8.9m\n",
            "[Epoch 10] Batch 3250/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 924.5s ETA: 8.6m\n",
            "[Epoch 10] Batch 3300/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 938.7s ETA: 8.4m\n",
            "[Epoch 10] Batch 3350/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 952.9s ETA: 8.2m\n",
            "[Epoch 10] Batch 3400/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 967.1s ETA: 7.9m\n",
            "[Epoch 10] Batch 3450/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 981.3s ETA: 7.7m\n",
            "[Epoch 10] Batch 3500/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 995.5s ETA: 7.5m\n",
            "[Epoch 10] Batch 3550/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 1009.7s ETA: 7.2m\n",
            "[Epoch 10] Batch 3600/5072 Loss: 0.0313 Acc: 0.987 Elapsed: 1023.9s ETA: 7.0m\n",
            "[Epoch 10] Batch 3650/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 1038.1s ETA: 6.7m\n",
            "[Epoch 10] Batch 3700/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 1052.3s ETA: 6.5m\n",
            "[Epoch 10] Batch 3750/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 1066.5s ETA: 6.3m\n",
            "[Epoch 10] Batch 3800/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 1080.7s ETA: 6.0m\n",
            "[Epoch 10] Batch 3850/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 1094.8s ETA: 5.8m\n",
            "[Epoch 10] Batch 3900/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 1109.0s ETA: 5.6m\n",
            "[Epoch 10] Batch 3950/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 1123.2s ETA: 5.3m\n",
            "[Epoch 10] Batch 4000/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 1137.4s ETA: 5.1m\n",
            "[Epoch 10] Batch 4050/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 1151.6s ETA: 4.8m\n",
            "[Epoch 10] Batch 4100/5072 Loss: 0.0316 Acc: 0.987 Elapsed: 1165.8s ETA: 4.6m\n",
            "[Epoch 10] Batch 4150/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 1180.0s ETA: 4.4m\n",
            "[Epoch 10] Batch 4200/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 1194.2s ETA: 4.1m\n",
            "[Epoch 10] Batch 4250/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 1208.4s ETA: 3.9m\n",
            "[Epoch 10] Batch 4300/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 1222.6s ETA: 3.7m\n",
            "[Epoch 10] Batch 4350/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 1236.7s ETA: 3.4m\n",
            "[Epoch 10] Batch 4400/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 1250.9s ETA: 3.2m\n",
            "[Epoch 10] Batch 4450/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 1265.1s ETA: 2.9m\n",
            "[Epoch 10] Batch 4500/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 1279.3s ETA: 2.7m\n",
            "[Epoch 10] Batch 4550/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 1293.5s ETA: 2.5m\n",
            "[Epoch 10] Batch 4600/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 1307.7s ETA: 2.2m\n",
            "[Epoch 10] Batch 4650/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 1321.9s ETA: 2.0m\n",
            "[Epoch 10] Batch 4700/5072 Loss: 0.0315 Acc: 0.987 Elapsed: 1336.1s ETA: 1.8m\n",
            "[Epoch 10] Batch 4750/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 1351.6s ETA: 1.5m\n",
            "[Epoch 10] Batch 4800/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 1365.9s ETA: 1.3m\n",
            "[Epoch 10] Batch 4850/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 1380.1s ETA: 1.1m\n",
            "[Epoch 10] Batch 4900/5072 Loss: 0.0314 Acc: 0.987 Elapsed: 1394.3s ETA: 0.8m\n",
            "[Epoch 10] Batch 4950/5072 Loss: 0.0313 Acc: 0.987 Elapsed: 1408.5s ETA: 0.6m\n",
            "[Epoch 10] Batch 5000/5072 Loss: 0.0313 Acc: 0.987 Elapsed: 1422.8s ETA: 0.3m\n",
            "[Epoch 10] Batch 5050/5072 Loss: 0.0313 Acc: 0.987 Elapsed: 1437.1s ETA: 0.1m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 10/10 [4:29:34<00:00, 1617.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 | Train: loss=0.0314, acc=0.987 | Val:   loss=0.0532, acc=0.981 | time=1565.6s (avg=1608.1s, ETA~0.0m) | ckpt→checkpoints/epoch_10.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Done. Final model saved as swin_large_final.pth\n"
          ]
        }
      ],
      "source": [
        "# ── 6) Transforms & DataLoaders ─────────────────────────────────────\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 128\n",
        "\n",
        "train_tf = T.Compose([\n",
        "    T.Resize(256),\n",
        "    T.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ColorJitter(0.2,0.2,0.2),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
        "])\n",
        "val_tf = T.Compose([\n",
        "    T.Resize(256),\n",
        "    T.CenterCrop(224),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "train_ds = WaferMapDataset(maps_train, lbls_train, transform=train_tf)\n",
        "val_ds   = WaferMapDataset(maps_val,   lbls_val,   transform=val_tf)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=batch_size, shuffle=True,\n",
        "    num_workers=16, pin_memory=True, persistent_workers=True, prefetch_factor=4\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_ds, batch_size=batch_size, shuffle=False,\n",
        "    num_workers=16, pin_memory=True, persistent_workers=True, prefetch_factor=4\n",
        ")\n",
        "\n",
        "print(f\"Train samples: {len(train_ds)}, batches: {len(train_loader)}\")\n",
        "print(f\"Val   samples: {len(val_ds)}, batches: {len(val_loader)}\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ── 7) Model, Loss, Optimizer, Scheduler, Scaler ───────────────────\n",
        "model = timm.create_model(\"swin_large_patch4_window7_224\", pretrained=True, num_classes=2).to(device)\n",
        "if hasattr(torch, \"compile\"):\n",
        "    model = torch.compile(model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Setup checkpoint directory\n",
        "ckpt_dir = \"checkpoints\"\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "# ── 8) Training loop with per-50-batch logs & checkpointing ───────\n",
        "epochs = 10\n",
        "epoch_times = []\n",
        "\n",
        "for epoch in trange(1, epochs+1, desc=\"Epochs\"):\n",
        "    t0 = time.time()\n",
        "\n",
        "    # — Train —\n",
        "    model.train()\n",
        "    t_loss, t_corr, t_total = 0.0, 0, 0\n",
        "    for batch_idx, (imgs, lbls) in enumerate(train_loader, start=1):\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        lbls = lbls.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            preds = model(imgs)\n",
        "            loss  = criterion(preds, lbls)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        bs = imgs.size(0)\n",
        "        t_loss  += loss.item() * bs\n",
        "        t_corr  += (preds.argmax(1)==lbls).sum().item()\n",
        "        t_total += bs\n",
        "\n",
        "        # every 50 batches, print interim stats & ETA\n",
        "        if batch_idx % 50 == 0:\n",
        "            elapsed = time.time() - t0\n",
        "            batches_done = batch_idx\n",
        "            batches_left = len(train_loader) - batches_done\n",
        "            eta = elapsed / batches_done * batches_left\n",
        "            print(\n",
        "                f\"[Epoch {epoch}] Batch {batch_idx}/{len(train_loader)} \"\n",
        "                f\"Loss: {t_loss/t_total:.4f} Acc: {t_corr/t_total:.3f} \"\n",
        "                f\"Elapsed: {elapsed:.1f}s ETA: {eta/60:.1f}m\"\n",
        "            )\n",
        "\n",
        "    train_loss = t_loss / t_total\n",
        "    train_acc  = t_corr / t_total\n",
        "\n",
        "    # — Validate —\n",
        "    model.eval()\n",
        "    v_loss, v_corr, v_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls in val_loader:\n",
        "            imgs = imgs.to(device, non_blocking=True)\n",
        "            lbls = lbls.to(device, non_blocking=True)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                preds = model(imgs)\n",
        "                loss  = criterion(preds, lbls)\n",
        "\n",
        "            bs = imgs.size(0)\n",
        "            v_loss  += loss.item() * bs\n",
        "            v_corr  += (preds.argmax(1)==lbls).sum().item()\n",
        "            v_total += bs\n",
        "\n",
        "    val_loss = v_loss / v_total\n",
        "    val_acc  = v_corr / v_total\n",
        "\n",
        "    # Step scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Timing & ETA for epochs\n",
        "    duration = time.time() - t0\n",
        "    epoch_times.append(duration)\n",
        "    avg_time = sum(epoch_times) / len(epoch_times)\n",
        "    eta      = avg_time * (epochs - epoch)\n",
        "\n",
        "    # Save checkpoint\n",
        "    ckpt_path = os.path.join(ckpt_dir, f\"epoch_{epoch}.pth\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict':     model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'scaler_state_dict':    scaler.state_dict(),\n",
        "        'train_loss':           train_loss,\n",
        "        'train_acc':            train_acc,\n",
        "        'val_loss':             val_loss,\n",
        "        'val_acc':              val_acc,\n",
        "        'epoch_time':           duration,\n",
        "    }, ckpt_path)\n",
        "\n",
        "    # Log summary\n",
        "    print(\n",
        "        f\"Epoch {epoch}/{epochs} | \"\n",
        "        f\"Train: loss={train_loss:.4f}, acc={train_acc:.3f} | \"\n",
        "        f\"Val:   loss={val_loss:.4f}, acc={val_acc:.3f} | \"\n",
        "        f\"time={duration:.1f}s (avg={avg_time:.1f}s, ETA~{eta/60:.1f}m) | \"\n",
        "        f\"ckpt→{ckpt_path}\"\n",
        "    )\n",
        "\n",
        "# ── 9) Final save ───────────────────────────────────────────────────\n",
        "torch.save(model.state_dict(), \"swin_large_final.pth\")\n",
        "print(\"✅ Done. Final model saved as swin_large_final.pth\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}